{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23391fa9-06a8-489d-9570-74081de4c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "game_files = [\n",
    "    name\n",
    "    for name in glob.glob(\"./z-machine-games-master/jericho-game-suite/*.z5\")\n",
    "    if \"zork1\" not in name\n",
    "]\n",
    "main_game_file = \"./z-machine-games-master/jericho-game-suite/zork1.z5\"\n",
    "\n",
    "train_model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\" #\"unsloth/llama-3-8b-bnb-4bit\"\n",
    "chat_template_name = \"llama-3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c83854d-2728-44cf-880f-b07ffcba30df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jericho\n",
    "\n",
    "\n",
    "def get_steps(filename: str):\n",
    "    env = jericho.FrotzEnv(filename)\n",
    "    \n",
    "    initial_obs, info = env.reset()\n",
    "    walkthrough = env.get_walkthrough()\n",
    "\n",
    "    steps = []\n",
    "    \n",
    "    obs = initial_obs\n",
    "    for step in walkthrough:\n",
    "        steps.append((obs, step))\n",
    "        #print(obs, step)\n",
    "        obs, reward, done, info = env.step(step)\n",
    "        #print(reward)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return steps\n",
    "\n",
    "\n",
    "steps = []\n",
    "for game_file in game_files:\n",
    "    steps.append(get_steps(game_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd80df7-b54c-4274-9ef6-a24282d04574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [{'content': '\\nWelcome to Adventure!\\n\\nMurdac\\nAn adventure game by Jonathan R. Partington (Cambridge University, 1982)\\n[This translation: version 1.111115 / Phoenix v1.04 / Inform v6.32\\nPlease type \"inform\" for further details.]\\n\\nWelcome to the Land of Murdac. This is version 1.07.\\n\\nType HELP for basic information, and BLURB for the full story.\\nAll comments to JRP1 please. New commands BRIEF/TERSE,\\nNORMAL/STANDARD, VERBOSE and EXAMINE have now been added.\\nYou are standing outside the door of a small flint hut.\\nThere are paths off to the east, west and south.\\nThe door is locked', 'role': 'user'}, {'content': 's', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "#from unsloth import standardize_sharegpt\n",
    "\n",
    "def steps_to_dataset(steps: list[list[tuple[str, str]]], length: int, overlap: bool = True):\n",
    "    convos = []\n",
    "\n",
    "    for game in steps:\n",
    "        convo = []\n",
    "        n = 0\n",
    "        \n",
    "        for step in game:\n",
    "            convo.append({\"role\": \"user\", \"content\": step[0]})\n",
    "            convo.append({\"role\": \"assistant\", \"content\": step[1]})\n",
    "            n += 1\n",
    "            if overlap:\n",
    "                if length > 0 and n > length:\n",
    "                    n -= 1\n",
    "                    convo.pop(0)\n",
    "                    convo.pop(0)\n",
    "                    \n",
    "                convos.append(list(convo))\n",
    "            else:\n",
    "                if length > 0 and n >= length:\n",
    "                    n = 0\n",
    "                    convos.append(convo)\n",
    "                    convo = []\n",
    "\n",
    "        if len(convo) > 0:\n",
    "            convos.append(convo)\n",
    "\n",
    "    return Dataset.from_dict({\"conversations\": convos})\n",
    "\n",
    "dataset = steps_to_dataset(steps, 5)\n",
    "print(dataset[0])\n",
    "#dataset = standardize_sharegpt(dataset)\n",
    "#print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "223d3b69-4da1-4a06-a1ff-c73f77b21641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-10 00:06:06 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 12-10 00:06:06 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.12.1: Fast Llama patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA RTX 4000 Ada Generation. Num GPUs = 1. Max memory: 19.548 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Your GPU cannot handle sequence lengths of 256 due to limited GPU memory.\n",
      "Unsloth: Your GPU can only handle approximately the maximum sequence length of 256.\n",
      "Unsloth: vLLM loading unsloth/llama-3.2-3b-instruct-bnb-4bit with actual GPU utilization = 3.47%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 19.55 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 256. Num Sequences = 8.\n",
      "Unsloth: vLLM's KV Cache can use up to 0.0 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 12-10 00:06:06 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 256, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.034747829236384524, 'max_num_batched_tokens': 2048, 'max_num_seqs': 8, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/llama-3.2-3b-instruct-bnb-4bit'}\n",
      "INFO 12-10 00:06:06 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
      "INFO 12-10 00:06:06 [__init__.py:1815] Using max model len 256\n",
      "INFO 12-10 00:06:07 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 12-10 00:06:07 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 12-10 00:06:07 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/llama-3.2-3b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.2-3b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/llama-3.2-3b-instruct-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":16,\"local_cache_dir\":null}\n",
      "INFO 12-10 00:06:08 [gpu_model_runner.py:2338] Starting to load model unsloth/llama-3.2-3b-instruct-bnb-4bit...\n",
      "INFO 12-10 00:06:09 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "Unsloth: Retrying vLLM to process 6 sequences and 2048 tokens in tandem.\n",
      "Error:\n",
      "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 19.55 GiB of which 9.12 MiB is free. Including non-PyTorch memory, this process has 19.52 GiB memory in use. Of the allocated memory 18.87 GiB is allocated by PyTorch, with 75.88 MiB allocated in private pools (e.g., CUDA Graphs), and 84.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "INFO 12-10 00:06:11 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 256, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.029535654850926844, 'max_num_batched_tokens': 2048, 'max_num_seqs': 6, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":16,\"local_cache_dir\":null}, 'model': 'unsloth/llama-3.2-3b-instruct-bnb-4bit'}\n",
      "INFO 12-10 00:06:12 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
      "INFO 12-10 00:06:12 [__init__.py:1815] Using max model len 256\n",
      "INFO 12-10 00:06:12 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 12-10 00:06:12 [scheduler.py:269] max_num_batched_tokens (2048) exceeds max_num_seqs * max_model_len (1536). This may lead to unexpected behavior.\n",
      "WARNING 12-10 00:06:12 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "WARNING 12-10 00:06:12 [scheduler.py:269] max_num_batched_tokens (2048) exceeds max_num_seqs * max_model_len (1536). This may lead to unexpected behavior.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 12-10 00:06:13 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/llama-3.2-3b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.2-3b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/llama-3.2-3b-instruct-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":16,\"local_cache_dir\":null}\n",
      "INFO 12-10 00:06:13 [gpu_model_runner.py:2338] Starting to load model unsloth/llama-3.2-3b-instruct-bnb-4bit...\n",
      "INFO 12-10 00:06:14 [gpu_model_runner.py:2370] Loading model from scratch...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Duplicate layer name: model.layers.0.self_attn.attn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/unsloth_zoo/vllm_utils.py:2090\u001b[39m, in \u001b[36mload_vllm\u001b[39m\u001b[34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs, use_bitsandbytes, unsloth_vllm_standby, is_vision_model, return_args, max_num_seqs)\u001b[39m\n\u001b[32m   2089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2090\u001b[39m     llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2091\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:282\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/engine/llm_engine.py:493\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    491\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:134\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    128\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    132\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    133\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:111\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:82\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInprocClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:245\u001b[39m, in \u001b[36mInprocClient.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/engine/core.py:82\u001b[39m, in \u001b[36mEngineCore.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, executor_fail_callback)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Setup Model.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m executor_fail_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/executor/executor_base.py:54\u001b[39m, in \u001b[36mExecutorBase.__init__\u001b[39m\u001b[34m(self, vllm_config)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m.observability_config = vllm_config.observability_config\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.is_sleeping = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/executor/uniproc_executor.py:49\u001b[39m, in \u001b[36mUniProcExecutor._init_executor\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.collective_rpc(\u001b[33m\"\u001b[39m\u001b[33minit_device\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mload_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/executor/uniproc_executor.py:58\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/utils/__init__.py:3060\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   3059\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3060\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_worker.py:213\u001b[39m, in \u001b[36mWorker.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._maybe_get_memory_pool_context(tag=\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43meep_scale_up\u001b[49m\u001b[43m=\u001b[49m\u001b[43meep_scale_up\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py:2371\u001b[39m, in \u001b[36mGPUModelRunner.load_model\u001b[39m\u001b[34m(self, eep_scale_up)\u001b[39m\n\u001b[32m   2370\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mLoading model from scratch...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2371\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mmodel_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lora_config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/model_loader/base_loader.py:45\u001b[39m, in \u001b[36mBaseModelLoader.load_model\u001b[39m\u001b[34m(self, vllm_config, model_config)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     model = \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLoading weights on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m, load_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/model_loader/utils.py:64\u001b[39m, in \u001b[36minitialize_model\u001b[39m\u001b[34m(vllm_config, prefix, model_class, model_config)\u001b[39m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_vllm_config(vllm_config,\n\u001b[32m     62\u001b[39m                                  check_compile=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     63\u001b[39m                                  prefix=prefix):\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m msg = (\u001b[33m\"\u001b[39m\u001b[33mvLLM model class should accept `vllm_config` and `prefix` as \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33minput arguments. Possibly you have an old-style model class\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33m registered from out of tree and it is used for new vLLM version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mCheck https://docs.vllm.ai/en/latest/design/arch_overview.html \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mfor the design and update the model class accordingly.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:537\u001b[39m, in \u001b[36mLlamaForCausalLM.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, layer_type)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28mself\u001b[39m.lora_config = lora_config\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:583\u001b[39m, in \u001b[36mLlamaForCausalLM._init_model\u001b[39m\u001b[34m(self, vllm_config, prefix, layer_type)\u001b[39m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init_model\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    580\u001b[39m                 vllm_config: VllmConfig,\n\u001b[32m    581\u001b[39m                 prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    582\u001b[39m                 layer_type: \u001b[38;5;28mtype\u001b[39m[nn.Module] = LlamaDecoderLayer):\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/compilation/decorators.py:199\u001b[39m, in \u001b[36m_support_torch_compile.<locals>.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, **kwargs)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, vllm_config: VllmConfig, prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[43mold_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mself\u001b[39m.vllm_config = vllm_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:359\u001b[39m, in \u001b[36mLlamaModel.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, layer_type)\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = PPMissingLayer()\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28mself\u001b[39m.start_layer, \u001b[38;5;28mself\u001b[39m.end_layer, \u001b[38;5;28mself\u001b[39m.layers = \u001b[43mmake_layers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.layers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/utils.py:643\u001b[39m, in \u001b[36mmake_layers\u001b[39m\u001b[34m(num_hidden_layers, layer_fn, prefix)\u001b[39m\n\u001b[32m    638\u001b[39m start_layer, end_layer = get_pp_indices(num_hidden_layers,\n\u001b[32m    639\u001b[39m                                         get_pp_group().rank_in_group,\n\u001b[32m    640\u001b[39m                                         get_pp_group().world_size)\n\u001b[32m    641\u001b[39m modules = torch.nn.ModuleList(\n\u001b[32m    642\u001b[39m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] + [\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m         maybe_offload_to_cpu(\u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    644\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[32m    645\u001b[39m     ] + [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:361\u001b[39m, in \u001b[36mLlamaModel.__init__.<locals>.<lambda>\u001b[39m\u001b[34m(prefix)\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = PPMissingLayer()\n\u001b[32m    359\u001b[39m \u001b[38;5;28mself\u001b[39m.start_layer, \u001b[38;5;28mself\u001b[39m.end_layer, \u001b[38;5;28mself\u001b[39m.layers = make_layers(\n\u001b[32m    360\u001b[39m     config.num_hidden_layers,\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m prefix: \u001b[43mlayer_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    365\u001b[39m     prefix=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.layers\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    366\u001b[39m )\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:276\u001b[39m, in \u001b[36mLlamaDecoderLayer.__init__\u001b[39m\u001b[34m(self, config, cache_config, quant_config, prefix)\u001b[39m\n\u001b[32m    274\u001b[39m     attn_type = AttentionType.ENCODER_ONLY\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[38;5;28mself\u001b[39m.self_attn = \u001b[43mLlamaAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_key_value_heads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_theta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrope_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias_o_proj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbias_o_proj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.self_attn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m.mlp = LlamaMLP(\n\u001b[32m    293\u001b[39m     hidden_size=\u001b[38;5;28mself\u001b[39m.hidden_size,\n\u001b[32m    294\u001b[39m     intermediate_size=config.intermediate_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m     prefix=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mlp\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:196\u001b[39m, in \u001b[36mLlamaAttention.__init__\u001b[39m\u001b[34m(self, config, hidden_size, num_heads, num_kv_heads, rope_theta, rope_scaling, max_position_embeddings, quant_config, bias, bias_o_proj, cache_config, prefix, attn_type)\u001b[39m\n\u001b[32m    193\u001b[39m attn_cls = (EncoderOnlyAttention\n\u001b[32m    194\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m attn_type == AttentionType.ENCODER_ONLY \u001b[38;5;28;01melse\u001b[39;00m Attention)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28mself\u001b[39m.attn = \u001b[43mattn_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_layer_sliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.attn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/attention/layer.py:198\u001b[39m, in \u001b[36mAttention.__init__\u001b[39m\u001b[34m(self, num_heads, head_size, scale, num_kv_heads, alibi_slopes, cache_config, quant_config, logits_soft_cap, per_layer_sliding_window, use_mla, prefix, attn_type, kv_sharing_target_layer_name, attn_backend, **extra_impl_args)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m compilation_config.static_forward_context:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDuplicate layer name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    199\u001b[39m compilation_config.static_forward_context[prefix] = \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Duplicate layer name: model.layers.0.self_attn.attn",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     37\u001b[39m     tokenizer = get_chat_template(tokenizer, chat_template = chat_template_name)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m model, tokenizer = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_model_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_name)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(model_name):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# required for vLLM, https://github.com/huggingface/open-r1/issues/572\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     model = FastLanguageModel.get_peft_model(\n\u001b[32m     24\u001b[39m         model,\n\u001b[32m     25\u001b[39m         r = \u001b[32m16\u001b[39m, \u001b[38;5;66;03m# any number > 0, suggested 8, 16, 32, 64, 128\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m         loftq_config = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# And LoftQ\u001b[39;00m\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     37\u001b[39m     tokenizer = get_chat_template(tokenizer, chat_template = chat_template_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/unsloth/models/loader.py:537\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, load_in_fp8, unsloth_tiled_mlp, *args, **kwargs)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fast_inference:\n\u001b[32m    535\u001b[39m     fast_inference, model_name = fast_inference_setup(model_name, model_config)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m model, tokenizer = \u001b[43mdispatch_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    561\u001b[39m     model.resize_token_embeddings(resize_model_vocab)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/unsloth/models/llama.py:2387\u001b[39m, in \u001b[36mFastLlamaModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, revision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, num_labels, qat_scheme, **kwargs)\u001b[39m\n\u001b[32m   2384\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2386\u001b[39m \u001b[38;5;66;03m# Load vLLM first\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2387\u001b[39m llm = \u001b[43mload_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mload_vllm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2389\u001b[39m \u001b[38;5;66;03m# Convert to HF format\u001b[39;00m\n\u001b[32m   2390\u001b[39m _, quant_state_dict = get_vllm_state_dict(llm, config = model_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/unsloth_zoo/vllm_utils.py:2104\u001b[39m, in \u001b[36mload_vllm\u001b[39m\u001b[34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs, use_bitsandbytes, unsloth_vllm_standby, is_vision_model, return_args, max_num_seqs)\u001b[39m\n\u001b[32m   2100\u001b[39m error = \u001b[38;5;28mstr\u001b[39m(error)\n\u001b[32m   2101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trials >= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m unsloth_vllm_standby:\n\u001b[32m   2102\u001b[39m     \u001b[38;5;66;03m# Sleep mode uses CuMemAllocator which can't run multiple instances in single process.\u001b[39;00m\n\u001b[32m   2103\u001b[39m     \u001b[38;5;66;03m# We can't do retry because vLLM will fail to load with said error.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error)\n\u001b[32m   2106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgpu_memory_utilization\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmemory\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error:\n\u001b[32m   2107\u001b[39m     approx_max_num_seqs = \u001b[38;5;28mint\u001b[39m(approx_max_num_seqs * \u001b[32m0.75\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Duplicate layer name: model.layers.0.self_attn.attn"
     ]
    }
   ],
   "source": [
    "# Taken from this article:\n",
    "# https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/tutorial-how-to-finetune-llama-3-and-use-in-ollama\n",
    "# https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Advanced_Llama3_2_(3B)_GRPO_LoRA.ipynb\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "lora_rank = 64 # Larger rank = smarter, but slower\n",
    "\n",
    "def load_model(model_name):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        fast_inference = True, # required for vLLM, https://github.com/huggingface/open-r1/issues/572\n",
    "        max_lora_rank = lora_rank,\n",
    "    )\n",
    "    \n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r = 16, # any number > 0, suggested 8, 16, 32, 64, 128\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                         \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha = 16,\n",
    "        lora_dropout = 0, # supports any, but 0 is optimized\n",
    "        bias = \"none\", # supports any, but \"none\" is optimized\n",
    "        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "        random_state = 3407,\n",
    "        use_rslora = False, # We support rank stabilized LoRA\n",
    "        loftq_config = None # And LoftQ\n",
    "    )\n",
    "    \n",
    "    tokenizer = get_chat_template(tokenizer, chat_template = chat_template_name)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model(train_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a5d126-807f-447c-bfb3-e4c9e817b604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class 'bytearray_iterator'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot pickle <class 'bytearray_iterator'>: builtins.bytearray_iterator has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Parameter 'function'=<function formatting_prompts_func at 0x7f2b1871afc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n",
      "[datasets.fingerprint|WARNING]Parameter 'function'=<function formatting_prompts_func at 0x7f2b1871afc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6664d22fc834216ad0aff5237945ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/datasets-guide\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt = False)\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {'text': texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "#dataset[0]\n",
    "#dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfc874d6-1545-4581-bd32-a3cf8cb29ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class 'bytearray_iterator'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot pickle <class 'bytearray_iterator'>: builtins.bytearray_iterator has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e64853496f9e40eb8838b47193895ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/7742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac738590accd49929d2c86f1058da511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=36):   0%|          | 0/7742 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "enable_bf16 = is_bfloat16_supported()\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences\n",
    "    formatting_func = formatting_prompts_func,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 120, #60,\n",
    "        # num_train_epochs = 1,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not enable_bf16,\n",
    "        bf16 = enable_bf16,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1089b039-b171-4fdf-b595-e63cb73da07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,742 | Num Epochs = 1 | Total steps = 120\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 02:41, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.229800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.904400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.183000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.511900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.739900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.379700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.524200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.552800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.887100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.776100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.341500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.375100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.708900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.204700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.525400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.869600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.252400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.130900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.792200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.924300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.968600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.282600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.117200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.845400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.221300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.697300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.960500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.972700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.247400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.070600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.015900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.995200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.143800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.535100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.768300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.897500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>1.652000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.634900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1.886700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>1.421800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.709300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1.883200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>1.515600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.284300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.981500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>1.778800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1.936300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.103500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>1.938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>1.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>1.676300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.185800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.326200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.916100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.157900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.929800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.328200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>1.849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>1.459100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1.757200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.997000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>1.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.436800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.177200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.900800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>1.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.092800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>1.896700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.128900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1.944900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1.896600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>1.390600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>2.177500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.742500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>1.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>2.030800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>1.840700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>1.783300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.981400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>2.083200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>1.813200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>1.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>2.145700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>2.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>1.781200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>1.506100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>1.805800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.899900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9d77698-9a11-4954-9703-39c197133ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/chat_template.jinja',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc7a3a61-365e-4daa-b345-52b005ab4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "\n",
    "def make_message(role, content):\n",
    "    return {\"role\": role, \"content\": content}\n",
    "\n",
    "\n",
    "def get_input_ids(messages):\n",
    "    return tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt = True,\n",
    "        return_tensors = \"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "\n",
    "def generate_response(messages):\n",
    "    input_ids = get_input_ids(messages)\n",
    "\n",
    "    output_ids = model.generate(input_ids,\n",
    "        max_new_tokens = 128,\n",
    "    )\n",
    "    out = tokenizer.batch_decode(output_ids)\n",
    "\n",
    "    #print(out)\n",
    "    \n",
    "    out_line = out[0]\n",
    "    start_token = \"<|end_header_id|>\"\n",
    "    end_token = tokenizer.eos_token\n",
    "    \n",
    "    start_index = out_line.rindex(start_token) + len(start_token)\n",
    "    end_index = out_line.rindex(end_token)\n",
    "    \n",
    "    return out_line[start_index : end_index].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "703bd1e2-f699-4484-aacd-f0531cc9b684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'open chest'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [make_message(\"user\", \"You are in a room. You see an egg on a table and a chest of drawers.\")]\n",
    "generate_response(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37008dd3-85b2-4c66-a72d-3f879114f602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get sword'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = [make_message(\"user\", \"You are in a cave. In front of you lies a sword sticking out of a large boulder.\")]\n",
    "generate_response(msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e8c0c00-e76a-405a-ad59-5c67c9a5bd6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\n",
      "ZORK is a registered trademark of Infocom, Inc.\n",
      "Revision 88 / Serial number 840726\n",
      "\n",
      "West of House\n",
      "You are standing in an open field west of a white house, with a boarded front door.\n",
      "There is a small mailbox here.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAIL\n",
      "You can't see any mail here!\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_game(game_filename: str, n_steps: int, with_history: bool):\n",
    "    env = jericho.FrotzEnv(game_filename)\n",
    "\n",
    "    messages = []\n",
    "    \n",
    "    obs, info = env.reset()\n",
    "    print(obs)\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        if not with_history:\n",
    "            messages.clear()\n",
    "\n",
    "        messages.append(make_message(\"user\", obs))\n",
    "        \n",
    "        response = generate_response(messages)\n",
    "        print(\">\", response)\n",
    "        messages.append(make_message(\"assistant\", response))\n",
    "        \n",
    "        obs, reward, done, info = env.step(response)\n",
    "        print(obs)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "run_game(main_game_file, 100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5a6df27-ffcf-4ccb-b41f-8257065f954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grpo_dataset(env: jericho.FrotzEnv):\n",
    "    states = {}\n",
    "    \n",
    "    initial_obs, info = env.reset()\n",
    "    walkthrough = env.get_walkthrough()\n",
    "\n",
    "    prompts = []\n",
    "    hashes = []\n",
    "    \n",
    "    obs = initial_obs\n",
    "    for step in walkthrough:\n",
    "        prompts.append([{\"role\": \"user\", \"content\": obs}])\n",
    "        \n",
    "        state_hash = env.get_world_state_hash()\n",
    "        hashes.append(state_hash)\n",
    "        if state_hash not in states:\n",
    "            states[state_hash] = env.get_state()\n",
    "        \n",
    "        obs, reward, done, info = env.step(step)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return Dataset.from_dict({\"prompt\": prompts, \"state_hashes\": hashes}), states\n",
    "\n",
    "\n",
    "env = jericho.FrotzEnv(main_game_file)\n",
    "grpo_dataset, states = make_grpo_dataset(env)\n",
    "\n",
    "\n",
    "# Workaround for segfault in FrotzEnv.set_state\n",
    "# Create + reuse a separate env within the GRPO reward function\n",
    "reward_env = None\n",
    "\n",
    "def shorten_response(response: str):\n",
    "    return response.split(\"\\n\")[0]\n",
    "\n",
    "def reward_func(prompts, completions, state_hashes, **kwargs): #(prompts, completions, **kwargs):\n",
    "    global reward_env\n",
    "    #env.set_state(states[state_hash])\n",
    "    #env.step\n",
    "    if reward_env is None:\n",
    "        reward_env = jericho.FrotzEnv(main_game_file)\n",
    "    #local_env = env.copy()\n",
    "    scores = []\n",
    "    for prompt, completion, state_hash in zip(prompts, completions, state_hashes):\n",
    "        reward_env.set_state(states[state_hash])\n",
    "        command = completion[0][\"content\"]\n",
    "\n",
    "        cur_inv_size = len(reward_env.get_inventory())\n",
    "        obs, reward, done, info = reward_env.step(command)\n",
    "        new_inv_size = len(reward_env.get_inventory())\n",
    "\n",
    "        if reward_env.get_world_state_hash() == state_hash:\n",
    "            # Punish taking an invalid action\n",
    "            reward -= 1.0\n",
    "        #if reward_env.get_world_state_hash() != state_hash:\n",
    "        #    # Reward taking a valid action\n",
    "        #    reward += 1.0\n",
    "        if new_inv_size > cur_inv_size:\n",
    "            # Reward picking up items\n",
    "            reward += new_inv_size - cur_inv_size\n",
    "\n",
    "        short_desc = shorten_response(prompt[0][\"content\"])\n",
    "        short_obs = shorten_response(obs)\n",
    "        #print(f\"'{short_desc}': '{command}' -> '{short_obs}' {reward}\")\n",
    "        #print(info)\n",
    "\n",
    "        scores.append(reward)\n",
    "    #local_env.close()\n",
    "\n",
    "    #print(scores)\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf62ff59-10ba-49d7-80d9-f55737a57c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo\n",
    "# https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Advanced_Llama3_2_(3B)_GRPO_LoRA.ipynb\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "max_seq_length = 2048\n",
    "max_prompt_length = 287 + 1\n",
    "\n",
    "grpo_training_args = GRPOConfig(\n",
    "    use_vllm = True, # use vLLM for fast inference!\n",
    "    learning_rate = 5e-6,\n",
    "    #adam_beta1 = 0.9,\n",
    "    #adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    #bf16 = is_bfloat16_supported(),\n",
    "    #fp16 = not is_bfloat16_supported(),\n",
    "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training, decrease if OOM\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_seq_length - max_prompt_length,\n",
    "    max_steps = 500,\n",
    "    save_steps = 250,\n",
    "    max_grad_norm = 1.0,\n",
    "    report_to = \"none\",\n",
    "    output_dir = \"outputs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4dff4411-07b1-4254-8e31-63fa333c2e05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-09 23:29:15 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 12-09 23:29:15 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.12.1: Fast Llama patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA RTX 4000 Ada Generation. Num GPUs = 1. Max memory: 19.548 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Your GPU cannot handle sequence lengths of 256 due to limited GPU memory.\n",
      "Unsloth: Your GPU can only handle approximately the maximum sequence length of 256.\n",
      "Unsloth: vLLM loading unsloth/llama-3.2-3b-instruct-bnb-4bit with actual GPU utilization = 10.56%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 19.55 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 256. Num Sequences = 8.\n",
      "Unsloth: vLLM's KV Cache can use up to 0.0 GB. Also swap space = 6 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 12-09 23:29:15 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 256, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.10558606714770559, 'max_num_batched_tokens': 2048, 'max_num_seqs': 8, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/llama-3.2-3b-instruct-bnb-4bit'}\n",
      "INFO 12-09 23:29:16 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
      "INFO 12-09 23:29:16 [__init__.py:1815] Using max model len 256\n",
      "INFO 12-09 23:29:16 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 12-09 23:29:16 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 12-09 23:29:17 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/llama-3.2-3b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.2-3b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/llama-3.2-3b-instruct-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":16,\"local_cache_dir\":null}\n",
      "INFO 12-09 23:29:17 [gpu_model_runner.py:2338] Starting to load model unsloth/llama-3.2-3b-instruct-bnb-4bit...\n",
      "INFO 12-09 23:29:18 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "INFO 12-09 23:29:18 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 12-09 23:29:18 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "INFO 12-09 23:29:18 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5d387d9b8a456a93fae2ef28385540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4296834a7604384983684b667a7a149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-09 23:29:19 [gpu_model_runner.py:2392] Model loading took 2.3205 GiB and 0.668185 seconds\n",
      "INFO 12-09 23:29:23 [backends.py:539] Using cache directory: /s/chopin/a/grad/elewark/.cache/vllm/torch_compile_cache/11a7d752eb/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 12-09 23:29:23 [backends.py:550] Dynamo bytecode transform time: 3.23 s\n",
      "INFO 12-09 23:29:27 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.394 s\n",
      "INFO 12-09 23:29:28 [monitor.py:34] torch.compile takes 3.23 s in total\n",
      "INFO 12-09 23:29:29 [gpu_worker.py:298] Available KV cache memory: -0.47 GiB\n",
      "Unsloth: Retrying vLLM to process 6 sequences and 2048 tokens in tandem.\n",
      "Error:\n",
      "No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.\n",
      "INFO 12-09 23:29:31 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 256, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'swap_space': 6, 'gpu_memory_utilization': 0.08974815707554974, 'max_num_batched_tokens': 2048, 'max_num_seqs': 6, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"/s/chopin/a/grad/elewark/.cache/vllm/torch_compile_cache/11a7d752eb\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":16,\"local_cache_dir\":\"/s/chopin/a/grad/elewark/.cache/vllm/torch_compile_cache/11a7d752eb/rank_0_0/backbone\"}, 'model': 'unsloth/llama-3.2-3b-instruct-bnb-4bit'}\n",
      "INFO 12-09 23:29:32 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n",
      "INFO 12-09 23:29:32 [__init__.py:1815] Using max model len 256\n",
      "INFO 12-09 23:29:32 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 12-09 23:29:32 [scheduler.py:269] max_num_batched_tokens (2048) exceeds max_num_seqs * max_model_len (1536). This may lead to unexpected behavior.\n",
      "WARNING 12-09 23:29:32 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "WARNING 12-09 23:29:32 [scheduler.py:269] max_num_batched_tokens (2048) exceeds max_num_seqs * max_model_len (1536). This may lead to unexpected behavior.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 12-09 23:29:33 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/llama-3.2-3b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.2-3b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=256, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/llama-3.2-3b-instruct-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"/s/chopin/a/grad/elewark/.cache/vllm/torch_compile_cache/11a7d752eb\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":16,\"local_cache_dir\":\"/s/chopin/a/grad/elewark/.cache/vllm/torch_compile_cache/11a7d752eb/rank_0_0/backbone\"}\n",
      "INFO 12-09 23:29:34 [gpu_model_runner.py:2338] Starting to load model unsloth/llama-3.2-3b-instruct-bnb-4bit...\n",
      "INFO 12-09 23:29:34 [gpu_model_runner.py:2370] Loading model from scratch...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Duplicate layer name: model.layers.0.self_attn.attn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/unsloth_zoo/vllm_utils.py:2090\u001b[39m, in \u001b[36mload_vllm\u001b[39m\u001b[34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs, use_bitsandbytes, unsloth_vllm_standby, is_vision_model, return_args, max_num_seqs)\u001b[39m\n\u001b[32m   2089\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2090\u001b[39m     llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2091\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/entrypoints/llm.py:282\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, override_pooler_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/engine/llm_engine.py:493\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers)\u001b[39m\n\u001b[32m    491\u001b[39m     engine_cls = V1LLMEngine\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:134\u001b[39m, in \u001b[36mLLMEngine.from_vllm_config\u001b[39m\u001b[34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_vllm_config\u001b[39m(\n\u001b[32m    128\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    132\u001b[39m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    133\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mLLMEngine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py:111\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:82\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mInprocClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/engine/core_client.py:245\u001b[39m, in \u001b[36mInprocClient.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m     \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/engine/core.py:82\u001b[39m, in \u001b[36mEngineCore.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, executor_fail_callback)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Setup Model.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28mself\u001b[39m.model_executor = \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m executor_fail_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/executor/executor_base.py:54\u001b[39m, in \u001b[36mExecutorBase.__init__\u001b[39m\u001b[34m(self, vllm_config)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m.observability_config = vllm_config.observability_config\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28mself\u001b[39m.is_sleeping = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/executor/uniproc_executor.py:49\u001b[39m, in \u001b[36mUniProcExecutor._init_executor\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mself\u001b[39m.collective_rpc(\u001b[33m\"\u001b[39m\u001b[33minit_device\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mload_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/executor/uniproc_executor.py:58\u001b[39m, in \u001b[36mUniProcExecutor.collective_rpc\u001b[39m\u001b[34m(self, method, timeout, args, kwargs)\u001b[39m\n\u001b[32m     57\u001b[39m     kwargs = {}\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m answer = \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/utils/__init__.py:3060\u001b[39m, in \u001b[36mrun_method\u001b[39m\u001b[34m(obj, method, args, kwargs)\u001b[39m\n\u001b[32m   3059\u001b[39m     func = partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3060\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_worker.py:213\u001b[39m, in \u001b[36mWorker.load_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._maybe_get_memory_pool_context(tag=\u001b[33m\"\u001b[39m\u001b[33mweights\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_runner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43meep_scale_up\u001b[49m\u001b[43m=\u001b[49m\u001b[43meep_scale_up\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py:2371\u001b[39m, in \u001b[36mGPUModelRunner.load_model\u001b[39m\u001b[34m(self, eep_scale_up)\u001b[39m\n\u001b[32m   2370\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mLoading model from scratch...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2371\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mmodel_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lora_config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/model_loader/base_loader.py:45\u001b[39m, in \u001b[36mBaseModelLoader.load_model\u001b[39m\u001b[34m(self, vllm_config, model_config)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     model = \u001b[43minitialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLoading weights on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m, load_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/model_loader/utils.py:64\u001b[39m, in \u001b[36minitialize_model\u001b[39m\u001b[34m(vllm_config, prefix, model_class, model_config)\u001b[39m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_current_vllm_config(vllm_config,\n\u001b[32m     62\u001b[39m                                  check_compile=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     63\u001b[39m                                  prefix=prefix):\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m msg = (\u001b[33m\"\u001b[39m\u001b[33mvLLM model class should accept `vllm_config` and `prefix` as \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33minput arguments. Possibly you have an old-style model class\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33m registered from out of tree and it is used for new vLLM version. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mCheck https://docs.vllm.ai/en/latest/design/arch_overview.html \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m        \u001b[33m\"\u001b[39m\u001b[33mfor the design and update the model class accordingly.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:537\u001b[39m, in \u001b[36mLlamaForCausalLM.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, layer_type)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28mself\u001b[39m.lora_config = lora_config\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_prefix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:583\u001b[39m, in \u001b[36mLlamaForCausalLM._init_model\u001b[39m\u001b[34m(self, vllm_config, prefix, layer_type)\u001b[39m\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init_model\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    580\u001b[39m                 vllm_config: VllmConfig,\n\u001b[32m    581\u001b[39m                 prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    582\u001b[39m                 layer_type: \u001b[38;5;28mtype\u001b[39m[nn.Module] = LlamaDecoderLayer):\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/compilation/decorators.py:199\u001b[39m, in \u001b[36m_support_torch_compile.<locals>.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, **kwargs)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, vllm_config: VllmConfig, prefix: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[43mold_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mself\u001b[39m.vllm_config = vllm_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:359\u001b[39m, in \u001b[36mLlamaModel.__init__\u001b[39m\u001b[34m(self, vllm_config, prefix, layer_type)\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = PPMissingLayer()\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[38;5;28mself\u001b[39m.start_layer, \u001b[38;5;28mself\u001b[39m.end_layer, \u001b[38;5;28mself\u001b[39m.layers = \u001b[43mmake_layers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.layers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/utils.py:643\u001b[39m, in \u001b[36mmake_layers\u001b[39m\u001b[34m(num_hidden_layers, layer_fn, prefix)\u001b[39m\n\u001b[32m    638\u001b[39m start_layer, end_layer = get_pp_indices(num_hidden_layers,\n\u001b[32m    639\u001b[39m                                         get_pp_group().rank_in_group,\n\u001b[32m    640\u001b[39m                                         get_pp_group().world_size)\n\u001b[32m    641\u001b[39m modules = torch.nn.ModuleList(\n\u001b[32m    642\u001b[39m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] + [\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m         maybe_offload_to_cpu(\u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    644\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[32m    645\u001b[39m     ] + [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:361\u001b[39m, in \u001b[36mLlamaModel.__init__.<locals>.<lambda>\u001b[39m\u001b[34m(prefix)\u001b[39m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = PPMissingLayer()\n\u001b[32m    359\u001b[39m \u001b[38;5;28mself\u001b[39m.start_layer, \u001b[38;5;28mself\u001b[39m.end_layer, \u001b[38;5;28mself\u001b[39m.layers = make_layers(\n\u001b[32m    360\u001b[39m     config.num_hidden_layers,\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m prefix: \u001b[43mlayer_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    365\u001b[39m     prefix=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.layers\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    366\u001b[39m )\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_pp_group().is_last_rank:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:276\u001b[39m, in \u001b[36mLlamaDecoderLayer.__init__\u001b[39m\u001b[34m(self, config, cache_config, quant_config, prefix)\u001b[39m\n\u001b[32m    274\u001b[39m     attn_type = AttentionType.ENCODER_ONLY\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m \u001b[38;5;28mself\u001b[39m.self_attn = \u001b[43mLlamaAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_key_value_heads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m                         \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_theta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrope_theta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_position_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias_o_proj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbias_o_proj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.self_attn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28mself\u001b[39m.mlp = LlamaMLP(\n\u001b[32m    293\u001b[39m     hidden_size=\u001b[38;5;28mself\u001b[39m.hidden_size,\n\u001b[32m    294\u001b[39m     intermediate_size=config.intermediate_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    298\u001b[39m     prefix=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.mlp\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/model_executor/models/llama.py:196\u001b[39m, in \u001b[36mLlamaAttention.__init__\u001b[39m\u001b[34m(self, config, hidden_size, num_heads, num_kv_heads, rope_theta, rope_scaling, max_position_embeddings, quant_config, bias, bias_o_proj, cache_config, prefix, attn_type)\u001b[39m\n\u001b[32m    193\u001b[39m attn_cls = (EncoderOnlyAttention\n\u001b[32m    194\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m attn_type == AttentionType.ENCODER_ONLY \u001b[38;5;28;01melse\u001b[39;00m Attention)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28mself\u001b[39m.attn = \u001b[43mattn_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_kv_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_layer_sliding_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43msliding_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.attn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/vllm/attention/layer.py:198\u001b[39m, in \u001b[36mAttention.__init__\u001b[39m\u001b[34m(self, num_heads, head_size, scale, num_kv_heads, alibi_slopes, cache_config, quant_config, logits_soft_cap, per_layer_sliding_window, use_mla, prefix, attn_type, kv_sharing_target_layer_name, attn_backend, **extra_impl_args)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m compilation_config.static_forward_context:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDuplicate layer name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    199\u001b[39m compilation_config.static_forward_context[prefix] = \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Duplicate layer name: model.layers.0.self_attn.attn",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model, tokenizer = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlora_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_name)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(model_name):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     model, tokenizer = \u001b[43mFastLanguageModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# required for vLLM, https://github.com/huggingface/open-r1/issues/572\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     model = FastLanguageModel.get_peft_model(\n\u001b[32m     24\u001b[39m         model,\n\u001b[32m     25\u001b[39m         r = \u001b[32m16\u001b[39m, \u001b[38;5;66;03m# any number > 0, suggested 8, 16, 32, 64, 128\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m         loftq_config = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# And LoftQ\u001b[39;00m\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     37\u001b[39m     tokenizer = get_chat_template(tokenizer, chat_template = chat_template_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/unsloth/models/loader.py:537\u001b[39m, in \u001b[36mFastLanguageModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, load_in_16bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, offload_embedding, float32_mixed_precision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, qat_scheme, load_in_fp8, unsloth_tiled_mlp, *args, **kwargs)\u001b[39m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fast_inference:\n\u001b[32m    535\u001b[39m     fast_inference, model_name = fast_inference_setup(model_name, model_config)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m model, tokenizer = \u001b[43mdispatch_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    538\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_get_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfix_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_patcher\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_peft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfast_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloat8_kv_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_lora_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resize_model_vocab \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    561\u001b[39m     model.resize_token_embeddings(resize_model_vocab)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/unsloth/models/llama.py:2387\u001b[39m, in \u001b[36mFastLlamaModel.from_pretrained\u001b[39m\u001b[34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, revision, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, unsloth_vllm_standby, num_labels, qat_scheme, **kwargs)\u001b[39m\n\u001b[32m   2384\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   2386\u001b[39m \u001b[38;5;66;03m# Load vLLM first\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2387\u001b[39m llm = \u001b[43mload_vllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mload_vllm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2389\u001b[39m \u001b[38;5;66;03m# Convert to HF format\u001b[39;00m\n\u001b[32m   2390\u001b[39m _, quant_state_dict = get_vllm_state_dict(llm, config = model_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cs542/cs542-adventure/venv/lib/python3.13/site-packages/unsloth_zoo/vllm_utils.py:2104\u001b[39m, in \u001b[36mload_vllm\u001b[39m\u001b[34m(model_name, config, gpu_memory_utilization, max_seq_length, dtype, training, float8_kv_cache, random_state, enable_lora, max_lora_rank, max_loras, use_async, use_engine, disable_log_stats, enforce_eager, enable_prefix_caching, compilation_config, conservativeness, max_logprobs, use_bitsandbytes, unsloth_vllm_standby, is_vision_model, return_args, max_num_seqs)\u001b[39m\n\u001b[32m   2100\u001b[39m error = \u001b[38;5;28mstr\u001b[39m(error)\n\u001b[32m   2101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trials >= \u001b[32m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m unsloth_vllm_standby:\n\u001b[32m   2102\u001b[39m     \u001b[38;5;66;03m# Sleep mode uses CuMemAllocator which can't run multiple instances in single process.\u001b[39;00m\n\u001b[32m   2103\u001b[39m     \u001b[38;5;66;03m# We can't do retry because vLLM will fail to load with said error.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(error)\n\u001b[32m   2106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mgpu_memory_utilization\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmemory\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error:\n\u001b[32m   2107\u001b[39m     approx_max_num_seqs = \u001b[38;5;28mint\u001b[39m(approx_max_num_seqs * \u001b[32m0.75\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Duplicate layer name: model.layers.0.self_attn.attn"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\"lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d53c134-8d20-4d7d-bcb6-757852a1f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://csolab.research.google.com/github/unslothai/notebooks/blob/main/nb/Advanced_Llama3_2_(3B)_GRPO_LoRA.ipynb\n",
    "\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        reward_func\n",
    "    ],\n",
    "    args = grpo_training_args,\n",
    "    train_dataset = grpo_dataset,\n",
    ")\n",
    "grpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3987da1-db64-4043-9b57-55b8433816b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\n",
      "ZORK is a registered trademark of Infocom, Inc.\n",
      "Revision 88 / Serial number 840726\n",
      "\n",
      "West of House\n",
      "You are standing in an open field west of a white house, with a boarded front door.\n",
      "There is a small mailbox here.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n",
      "> GET MAILBOX\n",
      "It is securely anchored.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_game(main_game_file, 100, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
