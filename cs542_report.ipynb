{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:21<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max unique_rooms: 13\n",
      "Mean unique_rooms: 9.85\n",
      "Max unique_hashes: 85\n",
      "Mean unique_hashes: 45.95\n",
      "Max unique_items: 6\n",
      "Mean unique_items: 2.35\n",
      "Max score: 15\n",
      "Mean score: -2.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "keys = ['unique_rooms', 'unique_hashes', 'unique_items', 'score']\n",
    "\n",
    "max_values = {}\n",
    "sums = {}\n",
    "\n",
    "sample_n = 20\n",
    "\n",
    "for i in tqdm(range(sample_n)):\n",
    "    values = run_random(env, print_output=False)\n",
    "    for k in keys:\n",
    "        value = values[k]\n",
    "        if value > max_values.get(k, -9999):\n",
    "            max_values[k] = value\n",
    "            \n",
    "        sums[k] = sums.get(k, 0) + value\n",
    "\n",
    "for k in keys:\n",
    "    print(f\"Max {k}:\", max_values.get(k, 0))\n",
    "    print(f\"Mean {k}:\", sums[k] / sample_n)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, the mean score here is below zero due to the random actions often causing the player to die."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qwen had the highest similarity on average, which makes sense intuitively from using it. It often described Zork as if it knew the game, even if it was often wrong in some way. The rest of the models still had high similarity, but each is working with the same starting text and the same system prompt, so it makes sense that the embeddings would be in close proximity in the latent space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import jericho\n",
    "\n",
    "def get_prompt(env: jericho.FrotzEnv, obs: str, done: bool, include_actions: bool = False):\n",
    "    \"\"\" Format a custom prompt including additional information about the environment. \"\"\"\n",
    "    \n",
    "    items=[\"##Observation\\n\" + obs]\n",
    "    state = env.get_state()\n",
    "\n",
    "    if not done:\n",
    "        look_desc, reward, done, info = env.step(\"look\")\n",
    "        if not obs.endswith(look_desc):\n",
    "            items.append(\"##Location\\n\" + look_desc)\n",
    "\n",
    "    if not done:\n",
    "        inv_desc, reward, done, info = env.step(\"inventory\")\n",
    "        items.append(\"##Inventory\\n\" + inv_desc)\n",
    "\n",
    "    if include_actions and not done:\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        bullets = [\"- \" + action for action in valid_actions]\n",
    "        items.append(\"##Available actions\\n\" + \"\\n\".join(bullets))\n",
    "\n",
    "    env.set_state(state)\n",
    "\n",
    "    return \"\\n\\n\".join(items)\n",
    "\n",
    "\n",
    "def get_steps(filename: str, extra_prompt = False):\n",
    "    \"\"\" Return a sequence of (prompt, action) pairs needed to complete a game. \"\"\"\n",
    "    env = jericho.FrotzEnv(filename)\n",
    "    \n",
    "    initial_obs, info = env.reset()\n",
    "    walkthrough = env.get_walkthrough()\n",
    "\n",
    "    steps = []\n",
    "   \n",
    "    done = False\n",
    "    obs = initial_obs\n",
    "    for step in walkthrough:\n",
    "        prompt = get_prompt(env, obs, done, include_actions=False)\n",
    "        steps.append((prompt, step))\n",
    "        obs, reward, done, info = env.step(step)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return steps\n",
    "\n",
    "\n",
    "def steps_to_dataset(steps: list[list[tuple[str, str]]], length: int, overlap: bool = True):\n",
    "    \"\"\"\n",
    "    Convert a sequence of game steps to a dataset of windowed conversations,\n",
    "    where the user prompt is the environment observation and the assistant response\n",
    "    is the command to execute.\n",
    "    \"\"\"\n",
    "    convos = []\n",
    "\n",
    "    for game in steps:\n",
    "        convo = []\n",
    "        n = 0\n",
    "        \n",
    "        for step in game:\n",
    "            convo.append({\"role\": \"user\", \"content\": step[0]})\n",
    "            convo.append({\"role\": \"assistant\", \"content\": step[1]})\n",
    "            n += 1\n",
    "            if overlap:\n",
    "                if length > 0 and n > length:\n",
    "                    n -= 1\n",
    "                    convo.pop(0)\n",
    "                    convo.pop(0)\n",
    "                    \n",
    "                convos.append(list(convo))\n",
    "            else:\n",
    "                if length > 0 and n >= length:\n",
    "                    n = 0\n",
    "                    convos.append(convo)\n",
    "                    convo = []\n",
    "\n",
    "        if len(convo) > 0:\n",
    "            convos.append(convo)\n",
    "\n",
    "    return Dataset.from_dict({\"conversations\": convos})\n",
    "\n",
    "\n",
    "def get_dataset(game_files: list[str], length: int, overlap: bool):\n",
    "    steps = []\n",
    "    for game_file in game_files:\n",
    "        steps.append(get_steps(game_file))\n",
    "    dataset = steps_to_dataset(steps, length=length, overlap=overlap)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def format_dataset(tokenizer, dataset):\n",
    "    \"\"\" Apply the model-specific chat template to a dataset. \"\"\"\n",
    "\n",
    "    # Based on the Unsloth for Llama3.2 notebook located here:\n",
    "    # https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb\n",
    "    def formatting_prompts_func(examples):\n",
    "        convos = examples[\"conversations\"]\n",
    "        texts = [\n",
    "            tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt = False)\n",
    "            for convo in convos\n",
    "        ]\n",
    "        return {'text': texts}\n",
    "\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can peek at this dataset to see what the output looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': '##Observation\\nCopyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\\nZORK is a registered trademark of Infocom, Inc.\\nRevision 88 / Serial number 840726\\n\\nWest of House\\nYou are standing in an open field west of a white house, with a boarded front door.\\nThere is a small mailbox here.\\n\\n\\n\\n##Inventory\\nYou are empty-handed.\\n\\n',\n",
       "   'role': 'user'},\n",
       "  {'content': 'N', 'role': 'assistant'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_files = [\"./z-machine-games-master/jericho-game-suite/zork1.z5\"]\n",
    "dataset = get_dataset(game_files, length=6, overlap=True)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the base Llama model to fine-tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 12-17 14:22:16 [__init__.py:216] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 12-17 14:22:20 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 12-17 14:22:20 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.12.1: Fast Llama patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA RTX 4000 Ada Generation. Num GPUs = 1. Max memory: 19.548 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/llama-3.2-3b-instruct-bnb-4bit with actual GPU utilization = 49.44%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 19.55 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 32.\n",
      "Unsloth: vLLM's KV Cache can use up to 7.11 GB. Also swap space = 4 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 12-17 14:22:23 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 2048, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'gpu_memory_utilization': 0.4944470289965936, 'max_num_batched_tokens': 4096, 'max_num_seqs': 32, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/llama-3.2-3b-instruct-bnb-4bit'}\n",
      "INFO 12-17 14:22:29 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 14:22:29 [__init__.py:1815] Using max model len 2048\n",
      "WARNING 12-17 14:22:29 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n",
      "INFO 12-17 14:22:31 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "WARNING 12-17 14:22:31 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 12-17 14:22:32 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/llama-3.2-3b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.2-3b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/llama-3.2-3b-instruct-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 12-17 14:22:32 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 12-17 14:22:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 12-17 14:22:32 [gpu_model_runner.py:2338] Starting to load model unsloth/llama-3.2-3b-instruct-bnb-4bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1217 14:22:32.573120415 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 14:22:32 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "INFO 12-17 14:22:32 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "INFO 12-17 14:22:33 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 12-17 14:22:33 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "INFO 12-17 14:22:33 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890e9e40c9d44878bbbd958dea31804c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95679c27e2be41df87f1f1721fed17e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 14:22:33 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 12-17 14:22:34 [gpu_model_runner.py:2392] Model loading took 2.3519 GiB and 1.103116 seconds\n",
      "INFO 12-17 14:22:39 [backends.py:539] Using cache directory: /s/chopin/a/grad/elewark/.cache/vllm/torch_compile_cache/dc2c8eddc4/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 12-17 14:22:39 [backends.py:550] Dynamo bytecode transform time: 4.96 s\n",
      "INFO 12-17 14:22:42 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.651 s\n",
      "INFO 12-17 14:22:43 [monitor.py:34] torch.compile takes 4.96 s in total\n",
      "INFO 12-17 14:22:44 [gpu_worker.py:298] Available KV cache memory: 6.91 GiB\n",
      "INFO 12-17 14:22:44 [kv_cache_utils.py:864] GPU KV cache size: 64,640 tokens\n",
      "INFO 12-17 14:22:44 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 31.56x\n",
      "INFO 12-17 14:22:44 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:01<00:00,  8.97it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 14:22:46 [gpu_model_runner.py:3118] Graph capturing finished in 2 secs, took 0.35 GiB\n",
      "INFO 12-17 14:22:46 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 2 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 14:22:47 [gpu_worker.py:391] Free memory on device (19.31/19.55 GiB) on startup. Desired GPU memory utilization is (0.4944470289965936, 9.67 GiB). Actual usage is 2.35 GiB for weight, 0.39 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.35 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=6881795584` to fit into requested memory, or `--kv-cache-memory=17234976256` to fully utilize gpu memory. Current kv cache memory in use is 7414472192 bytes.\n",
      "INFO 12-17 14:22:47 [core.py:218] init engine (profile, create kv cache, warmup model) took 13.09 seconds\n",
      "INFO 12-17 14:22:48 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 12-17 14:22:48 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'input_layernorm', 'layer_norm1', 'layer_norm2', 'post_layernorm', 'pre_feedforward_layernorm', 'k_norm', 'norm1', 'norm', 'ffn_norm', 'post_attention_layernorm', 'norm2', 'post_feedforward_layernorm', 'attention_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at unsloth/llama-3.2-3b-instruct-bnb-4bit and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'input_layernorm', 'layer_norm1', 'cross_attn_post_attention_layernorm', 'layer_norm2', 'post_layernorm', 'cross_attn_input_layernorm', 'pre_feedforward_layernorm', 'k_norm', 'norm1', 'norm', 'ffn_norm', 'post_attention_layernorm', 'norm2', 'post_feedforward_layernorm', 'attention_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.12.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from adventure.model import load_model, save_model\n",
    "\n",
    "model, tokenizer = load_model(\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", \"llama-3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, applying the model-specific formatter adds a 'text' attribute containing the actual content the Llama model will see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class 'bytearray_iterator'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot pickle <class 'bytearray_iterator'>: builtins.bytearray_iterator has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Parameter 'function'=<function format_dataset.<locals>.formatting_prompts_func at 0x7f5251ecf420> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n",
      "[datasets.fingerprint|WARNING]Parameter 'function'=<function format_dataset.<locals>.formatting_prompts_func at 0x7f5251ecf420> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957b8e76d45d4931adf5882ec0c436be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': '##Observation\\nCopyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\\nZORK is a registered trademark of Infocom, Inc.\\nRevision 88 / Serial number 840726\\n\\nWest of House\\nYou are standing in an open field west of a white house, with a boarded front door.\\nThere is a small mailbox here.\\n\\n\\n\\n##Inventory\\nYou are empty-handed.\\n\\n',\n",
       "   'role': 'user'},\n",
       "  {'content': 'N', 'role': 'assistant'}],\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n##Observation\\nCopyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\\nZORK is a registered trademark of Infocom, Inc.\\nRevision 88 / Serial number 840726\\n\\nWest of House\\nYou are standing in an open field west of a white house, with a boarded front door.\\nThere is a small mailbox here.\\n\\n\\n\\n##Inventory\\nYou are empty-handed.\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nN<|eot_id|>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = format_dataset(tokenizer, dataset)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After producing the dataset from the game environment, we can fine-tune the model using Unsloth as follows.\n",
    "The boilerplate logic for fine-tuning is located under the modules directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class 'bytearray_iterator'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot pickle <class 'bytearray_iterator'>: builtins.bytearray_iterator has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d9f01cc9d54836bcd803af9c684b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7499205e0f54f2ba1ae3e9c95585d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=36):   0%|          | 0/397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 397 | Num Epochs = 1 | Total steps = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.795600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.382400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.751400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.600800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.804000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# More detailed code for fine-tuning and model saving/loading is included in these modules.\n",
    "# That code is based on this tutorial and notebook from Unsloth.\n",
    "# https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/tutorial-how-to-finetune-llama-3-and-use-in-ollama\n",
    "# https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb\n",
    "from adventure.finetune import make_trainer\n",
    "\n",
    "trainer = make_trainer(model, tokenizer, dataset, max_steps=10) # Limit number of steps for this example\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "#save_model(model, tokenizer, \"lora_model_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\n",
      "ZORK is a registered trademark of Infocom, Inc.\n",
      "Revision 88 / Serial number 840726\n",
      "\n",
      "West of House\n",
      "You are standing in an open field west of a white house, with a boarded front door.\n",
      "There is a small mailbox here.\n",
      "\n",
      "\n",
      "> E\n",
      "The door is boarded and you can't remove the boards.\n",
      "\n",
      "\n",
      "> E\n",
      "The door is boarded and you can't remove the boards.\n",
      "\n",
      "\n",
      "{'moves': 2, 'unique_rooms': 1, 'unique_hashes': 1, 'unique_items': 0, 'score': 0, 'max_score': 350, 'avg_retries': 1.0, 'avg_generate_time': 0.09915399551391602}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'moves': 2,\n",
       " 'unique_rooms': 1,\n",
       " 'unique_hashes': 1,\n",
       " 'unique_items': 0,\n",
       " 'score': 0,\n",
       " 'max_score': 350,\n",
       " 'avg_retries': 1.0,\n",
       " 'avg_generate_time': 0.09915399551391602}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "from adventure.player import run_game\n",
    "run_game(model, tokenizer, \"./z-machine-games-master/jericho-game-suite/zork1.z5\", 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fine-tuning process is effective at training the model to output commands in the correct format. When trained for more steps than seen here, the model even carries out some correct instructions from the game walkthrough. However, this still does not allow the model to get very far through the game.\n",
    "\n",
    "The main issue here is that this process does not necessarily teach the model how to decide its next action based on the state of the environment. Any deviation from the path it was taught can then compound further since it was not shown how to handle the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3UCQGB2pjKb"
   },
   "source": [
    "## Retrieval Augmented Generation\n",
    "\n",
    "A recurring issue with language models is that it can be more difficult for them to â€œconnect the dotsâ€ between different observations over the course of the game. Additionally, game worlds can be quite large in size, to the extent where the context size may not be sufficient to capture what information is specifically relevant to a scenario the model encounters.\n",
    "\n",
    "One potential avenue to address these deficiencies is Retrieval Augmented Generation (RAG), which allows a model to search a vector database to find documents similar to a set of keywords. These similarities are computed based on embeddings generated by a separate model. RAG can be further augmented into a graph-based system that allows links between individual documents. This allows more complex relationships to be described, which is useful in this task. For example, edges can describe pathways between locations or objects contained within.\n",
    "\n",
    "Langchain is a framework that can wrap other LLM providers such as Ollama within an API designed for â€œagentâ€-based flows. Here, a model uses sequences of tool actions to carry out some task.\n",
    "\n",
    "Vending-Bench: https://arxiv.org/abs/2502.15840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.documents import Document\n",
    "from ollama import ResponseError\n",
    "\n",
    "def rag_agent():\n",
    "    game_response, info = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    @tool(response_format=\"content\")\n",
    "    def do_game_action(action: str) -> str:\n",
    "        \"\"\"Perform an action in the active text adventure game and see the result\"\"\"\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          action: game action string\n",
    "\n",
    "        Returns:\n",
    "          The game's response after performing the action\n",
    "        \"\"\"\n",
    "        nonlocal done, game_response, info\n",
    "        game_response, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            game_response += '\\nYou have finished the game!'\n",
    "        return game_response\n",
    "    \n",
    "    @tool(response_format=\"content\")\n",
    "    def view_possible_actions() -> str:\n",
    "        \"\"\"View a list of the actions that can be performed in the game's current state\"\"\"\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          String containg actions separated by commas\n",
    "        \"\"\"\n",
    "        return ', '.join(env.get_valid_actions())        \n",
    "\n",
    "\n",
    "    tools = [remember, do_game_action, view_possible_actions]\n",
    "    system_prompt = (\n",
    "        f\"You are playing {game}, an interactive fiction game. You must analyze the scenario the game presents to you and choose an action that will make progress. Your goal is to finish the game\\n\"\n",
    "        \"You have access to a tool that allows you to remember past events that have occured in your current playthrough that are relevant to your situation. \"\n",
    "        \"Use the tool to help you decide on the next action to take in-game \"\n",
    "    )\n",
    "    agent = create_agent(model, tools, system_prompt=system_prompt)\n",
    "    \n",
    "    def agent_stream():\n",
    "        nonlocal agent, game_response\n",
    "        query = (\n",
    "            \"Think critically. Finish the game.\\n\"\n",
    "            # f\"Here are relevant items from your past moves:\\n{remember.invoke({'query':game_response})}\\n\"\n",
    "            f\"Here is your current scenario:\\n{game_response}\"\n",
    "        )\n",
    "        try:\n",
    "            for event in agent.stream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "                stream_mode=\"values\",\n",
    "            ):\n",
    "                last_message = event[\"messages\"][-1]\n",
    "                last_message.pretty_print()\n",
    "                \n",
    "                document = Document(\n",
    "                    page_content=last_message.content, metadata={\"move\": info['moves']}\n",
    "                )\n",
    "                insert_into_vector_store([document])\n",
    "                yield None\n",
    "        except ResponseError:\n",
    "            print('ResponseError occurred')\n",
    "            \n",
    "    cur_stream = None\n",
    "    def turn():\n",
    "        nonlocal cur_stream, done, info\n",
    "        if cur_stream is None:\n",
    "            cur_stream = agent_stream()\n",
    "        try:\n",
    "            next(cur_stream)\n",
    "        except StopIteration:\n",
    "            cur_stream = agent_stream()\n",
    "        return done, info\n",
    "    \n",
    "    results = n_steps(turn, env, 10)\n",
    "    print(results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws_oMb1GqSzS"
   },
   "source": [
    "## Graph-based RAG\n",
    "\n",
    "As seen with Q-BERT, knowledge graphs can be useful to capture the overall state of a game. Individual locations can be represented as nodes on a graph, with directed edges between them indicating pathways. The graph can also link objects within these rooms. By representing connections in this way, spatial relationships can be captured in a way that is accessible to search algorithms, while abstracting away their complexity from the model itself.\n",
    "\n",
    "We used LangChain's Graph RAG retriever, supported by the DataStax [graph-rag](https://datastax.github.io/graph-rag/) library, to use our existing in-memory LangChain vector store as the knowledge graph where edges are defined by commonly keyed metadata fields. The model still provides information about its current game state to the retrieval function, but the response includes the retrieved documents and k connected documents across graph edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adventure.rag_bot import Game as RagGame\n",
    "game = Game()\n",
    "game.run_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Existing Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_copyright = '''West of House\n",
    "You are standing in an open field west of a white house, with a boarded front door.\n",
    "There is a small mailbox here.\n",
    "'''\n",
    "actual = embedding(without_copyright)\n",
    "\n",
    "first_half = '''West of House\n",
    "You are standing in an open field west of\n",
    "'''\n",
    "\n",
    "prompt = f'Below is the first half of the very first text prompt given by the text adventure game Zork. Please complete it exactly as it is given by Zork. Do not include anything except your completion.\\n\\n\"{first_half}\" (completion here...)'\n",
    "response = client.responses.create(\n",
    "        model=\"gpt-5.2\",\n",
    "        input=prompt\n",
    "    )\n",
    "reply = response.output_text\n",
    "rebuilt = first_half + ' ' + reply.split(':')[-1]\n",
    "print(rebuilt)\n",
    "cosine_similarity(embedding(rebuilt), actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-5.2's reconstructed Zork introduction text has the highest cosine similarity of all, often generating it exactly the same as the game. This indicates that its good performance could be linked to it already having memorized Zork or much of Zork, which means it may not be employing any actual reasoning when playing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "| model | Unique Rooms | Unique Items | Unique Game States | Score | Average Retries | Average Generate Time |\n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "||||||||\n",
    "| Basic LLaMa | 6 | 1 | 13 | 0 | 100+ | 1.85s |\n",
    "| Memory | 5 | 0 | 8 | 10 | 30 | 1.38s |\n",
    "| Memory & ChatGPT Prompt | 8 | 1 | 11 | 0 | 100+ | 0.255s |\n",
    "| Memory & Provided Commands | 8 | 5 | 53 | 10 | 6 | 0.15s |\n",
    "| Memory, Analyze, Provided Commands | 10 | 7 | 60 | 10 | 15.5 | 1.86s |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZHmg9qXmdPF"
   },
   "source": [
    "# Text Adventures and LLMs\n",
    "\n",
    "**Everett Lewark & Tyson O'Leary**\n",
    "\n",
    "CS 542: Natural Language Processing\n",
    "\n",
    "Dec. 17, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0wnw7CGsXyX"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Video games and artificial intelligence have long overlapped, even as the popular perception of what technologies constitute AI has shifted over time. Categories of video game AI vary from simpler behaviors like those of arcade-game enemies to more advanced state-machine approaches or even the strategic search algorithms seen in chess engines. However, as players often face off against computer-controlled opponents in these games, a question arises: in what cases can these algorithms take the role of players themselves?\n",
    "\n",
    "Prior research in machine learning has investigated applications of machine learning to the process of playing video games. This process is aided by the fact that video games are self-contained, easily repeatable, and can be configured to be deterministic if controlling the state of the random number generator. Within the scope of natural language processing, there has been some work on applying language models to text adventure games. Typically, these games task players with navigating around multi-room environments, locating and collecting various objects, and using these objects to solve puzzles and unlock new areas. This presents a challenging task for language models, since they have to identify these objects and interact with the environment to progress through the game.\n",
    "\n",
    "The [Jericho](https://github.com/microsoft/jericho) and [TextWorld](https://github.com/Microsoft/TextWorld) projects from Microsoft provide text-adventure environments to use with language models, and we used the former as our test bench for our models. Many classic adventure games, including Zork (our chosen environment), [are available](https://github.com/BYU-PCCL/z-machine-games) in a format compatible with Jericho."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lccvznglms6q"
   },
   "source": [
    "## Random baseline\n",
    "\n",
    "In a prior project, we found that a model trained to play Street Fighter was actually comparable to one that just input actions randomly. Following that pattern, we chose to implement a naÃ¯ve model here that simply picked random actions out of the set of possible commands. As in the Street Fighter case, this baseline model is also useful to understand how well an LLM performs at a text adventure game.\n",
    "\n",
    "To evaluate all of our models, we track a variety of properties about the game environment as we step through:\n",
    "- Unique rooms reached\n",
    "- Unique items gathered\n",
    "- Unique game states (hashes)\n",
    "- Score (as reported by the game)\n",
    "- Average moves (\"retries\") between increases in score\n",
    "\n",
    "Many classic text adventure games use a custom Z-Machine language. This allowed games to be programmed once and then run on many different computers using platform-specific interpreters, thereby drastically reducing the complexity of porting them.\n",
    "\n",
    "Jericho incorporates a Z-Machine interpreter called Frotz to run these games. Using this library, let's run our random agent through 100 turns of Zork to see how it performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Kc6k4Zd4m_aQ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\n",
      "ZORK is a registered trademark of Infocom, Inc.\n",
      "Revision 88 / Serial number 840726\n",
      "\n",
      "West of House\n",
      "You are standing in an open field west of a white house, with a boarded front door.\n",
      "There is a small mailbox here.\n",
      "\n",
      "\n",
      "> north\n",
      "North of House\n",
      "You are facing the north side of a white house. There is no door here, and all the windows are boarded up. To the north a narrow path winds through the trees.\n",
      "\n",
      "\n",
      "> north\n",
      "Forest Path\n",
      "This is a path winding through a dimly lit forest. The path heads north-south here. One particularly large tree with some low branches stands at the edge of the path.\n",
      "\n",
      "\n",
      "[...trimmed...]\n",
      "> south\n",
      "Forest\n",
      "There is a bird's nest here.\n",
      "\n",
      "\n",
      "> northwest\n",
      "South of House\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'moves': 100,\n",
       " 'unique_rooms': 8,\n",
       " 'unique_hashes': 67,\n",
       " 'unique_items': 4,\n",
       " 'score': 0,\n",
       " 'max_score': 350,\n",
       " 'avg_retries': 1.0,\n",
       " 'avg_generate_time': 0.015129828453063964}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-2373:\n",
      "Process ForkPoolWorker-2372:\n",
      "Process ForkPoolWorker-2353:\n",
      "Process ForkPoolWorker-2370:\n",
      "Process ForkPoolWorker-2360:\n",
      "Process ForkPoolWorker-2367:\n",
      "Process ForkPoolWorker-2361:\n",
      "Process ForkPoolWorker-2363:\n",
      "Process ForkPoolWorker-2365:\n",
      "Process ForkPoolWorker-2368:\n",
      "Process ForkPoolWorker-2357:\n",
      "Process ForkPoolWorker-2369:\n",
      "Process ForkPoolWorker-2355:\n",
      "Process ForkPoolWorker-2371:\n",
      "Process ForkPoolWorker-2349:\n",
      "Process ForkPoolWorker-2366:\n",
      "Process ForkPoolWorker-2358:\n",
      "Process ForkPoolWorker-2347:\n",
      "Process ForkPoolWorker-2346:\n",
      "Process ForkPoolWorker-2354:\n",
      "Process ForkPoolWorker-2359:\n",
      "Process ForkPoolWorker-2356:\n",
      "Process ForkPoolWorker-2351:\n",
      "Process ForkPoolWorker-2364:\n",
      "Process ForkPoolWorker-2362:\n",
      "Process ForkPoolWorker-2345:\n",
      "Process ForkPoolWorker-2344:\n",
      "Process ForkPoolWorker-2352:\n",
      "Process ForkPoolWorker-2348:\n",
      "Process ForkPoolWorker-2343:\n",
      "Process ForkPoolWorker-2342:\n",
      "Process ForkPoolWorker-2350:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "Traceback (most recent call last):\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 313, in _bootstrap\n",
      "    self.run()\n",
      "    ~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 385, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/queues.py\", line 384, in get\n",
      "    with self._rlock:\n",
      "         ^^^^^^^^^^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "KeyboardInterrupt\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "KeyboardInterrupt\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "KeyboardInterrupt\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/s/chopin/a/grad/elewark/python/Python-3.13.9/Lib/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from jericho import FrotzEnv\n",
    "\n",
    "# Import our score tracking code from our main module\n",
    "# To see more, check out the code under the 'adventure' directory\n",
    "from adventure.metrics import ScoreTracker\n",
    "\n",
    "STORY_FILE = \"./z-machine-games-master/jericho-game-suite/zork1.z5\"\n",
    "\n",
    "def run_random(env, print_output=True):\n",
    "    obs, info = env.reset()\n",
    "    if print_output:\n",
    "        print(obs)\n",
    "    \n",
    "    score_tracker = ScoreTracker(env)\n",
    "    \n",
    "    for i in range(100):\n",
    "        start_time = time.time()\n",
    "        actions = env.get_valid_actions()\n",
    "        action = random.choice(actions)\n",
    "        end_time = time.time()\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        score_tracker.update(info, start_time, end_time)\n",
    "\n",
    "        if print_output and (i <= 1 or done or i >= 98):\n",
    "            print(\">\", action)\n",
    "            print(obs)\n",
    "            if i == 1:\n",
    "                print(\"[...trimmed...]\")\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return score_tracker.get_stats(env, info, print_output=False)\n",
    "\n",
    "\n",
    "env = FrotzEnv(STORY_FILE)\n",
    "run_random(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnFDwSZ6m2VU"
   },
   "source": [
    "The random algorithm's performance was typically worse than that observed with LLMs, but unpredictably it could receive higher scores. Often, the model would jump off a cliff or be eaten by a grue, rather than simply running out of turns as the others do.\n",
    "\n",
    "To inspect the variability of these metrics, we can run the environment multiple times and aggregate the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:21<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max unique_rooms: 13\n",
      "Mean unique_rooms: 9.85\n",
      "Max unique_hashes: 85\n",
      "Mean unique_hashes: 45.95\n",
      "Max unique_items: 6\n",
      "Mean unique_items: 2.35\n",
      "Max score: 15\n",
      "Mean score: -2.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "keys = ['unique_rooms', 'unique_hashes', 'unique_items', 'score']\n",
    "\n",
    "max_values = {}\n",
    "sums = {}\n",
    "\n",
    "sample_n = 20\n",
    "\n",
    "for i in tqdm(range(sample_n)):\n",
    "    values = run_random(env, print_output=False)\n",
    "    for k in keys:\n",
    "        value = values[k]\n",
    "        if value > max_values.get(k, -9999):\n",
    "            max_values[k] = value\n",
    "            \n",
    "        sums[k] = sums.get(k, 0) + value\n",
    "\n",
    "for k in keys:\n",
    "    print(f\"Max {k}:\", max_values.get(k, 0))\n",
    "    print(f\"Mean {k}:\", sums[k] / sample_n)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, the mean score here is below zero due to the random actions often causing the player to die."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7fwz4j3nfEJ"
   },
   "source": [
    "## Off-the-shelf models\n",
    "\n",
    "Starting simple, we chose to assess the ability of off-the-shelf open-weight models to play Zork. We start with LLaMa3.2 1b and 3b as the language model acting as the player. Our first strategy simply provides a system prompt informing the model that it is playing a text adventure game, then prompts the model with the gameâ€™s scenario description for a response, which is fed back into the game as an action to take. This is intentionally naive, and the results reflect it. The model is inconsistent with the format and validity of the actions it gives. If a valid action is â€œopen mailboxâ€, model responses could vary from â€œmailâ€ to â€œAs the player, you should type: open the mailbox and read its contentsâ€. Neither of these are recognized by the game; however a human player could make these same mistakes when learning how commands are written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnmHHoSMn946"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import ollama\n",
    "import jericho\n",
    "import re\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xCz0vmmJ2mPe"
   },
   "outputs": [],
   "source": [
    "game = 'zork1.z5'\n",
    "GAMES_DIR = \"z-machine-games-master/jericho-game-suite\"\n",
    "env = jericho.FrotzEnv(f\"{GAMES_DIR}/{game}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywesEHzW2_DC"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from adventure.metrics import ScoreTracker\n",
    "\n",
    "def n_steps(turn_func, env, n=100):\n",
    "    score_tracker = ScoreTracker(env)\n",
    "\n",
    "    for _ in range(n):\n",
    "\n",
    "        # Turn\n",
    "        start = time.time()\n",
    "        done, info = turn_func()\n",
    "        end = time.time()\n",
    "\n",
    "        score_tracker.update(info, start, end)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return score_tracker.get_stats(env, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZVXGJCy93BXW"
   },
   "outputs": [],
   "source": [
    "def basic_llm():\n",
    "    # Basic\n",
    "    system_prompt = (\n",
    "        'You are a smart video game enthusiast who is skilled at playing old-school text adventure games. Given a description of your environment, explain your thought process and then give a command that is compatible with the game you are playing. Always put the command on its own line at the end of your response with nothing else. It needs to be easy and consistent to read with simple python'\n",
    "        'This game does not run on an LLM, so it only recognizes a small vocabulary of commands. An example of a command is exactly the string \"go west\". This means you should only give exactly the command that the game recognizes '\n",
    "    )\n",
    "\n",
    "    make_prompt = lambda x: f'{system_prompt}\\n\\n{x}'\n",
    "\n",
    "    game_response, info = env.reset()\n",
    "    print(game_response)\n",
    "\n",
    "\n",
    "    def turn():\n",
    "        nonlocal game_response\n",
    "        prompt = make_prompt(f'Game prompt:\\n{game_response}')\n",
    "        response = ollama.generate(model='llama3.2:1b', prompt=prompt).response\n",
    "        print(f'LLM Response: {response}')\n",
    "        player_in = response.splitlines()[-1].strip()\n",
    "        print(player_in)\n",
    "\n",
    "        game_response, reward, done, info = env.step(player_in)\n",
    "        print(game_response)\n",
    "        return done, info\n",
    "\n",
    "    results = n_steps(turn, env)\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbPeGq0Lnt6J"
   },
   "source": [
    "Next, we built on the first strategy by adding a memory for the LLM, which provides all of the modelâ€™s past game interactions in the prompt. This was a step in the right direction because the model would sometimes recognize that the previous action wasnâ€™t recognized, but it still was unable to fix the problem very often. This ability to â€œretryâ€ shows better promise in other strategies we employ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMAfjmvxn-dd"
   },
   "outputs": [],
   "source": [
    "# Basic with memory\n",
    "system_prompt_memory = (\n",
    "    'You are a smart video game enthusiast who is skilled at playing old-school text adventure games. Given a description of your environment, explain your thought process and then give a command that is compatible with the game you are playing. Always put the command on its own line at the end of your response with nothing else. It needs to be easy and consistent to read with simple python'\n",
    "    'At the beginning of your prompt, you will also receive up to 5 of the most recent interactions you\\'ve had with the game'\n",
    "    'This game does not run on an LLM, so it only recognizes a set vocabulary of commands. An example of a command is exactly the string \"go west\". This means you should only give exactly the command that the game recognizes '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-o9SNY23S-E"
   },
   "outputs": [],
   "source": [
    "def basic_llm_with_memory(memory_size=5, system_prompt=system_prompt_memory):\n",
    "\n",
    "    make_prompt = lambda x: f'{system_prompt}\\n\\n{x}'\n",
    "\n",
    "    game_response, info = env.reset()\n",
    "    print(game_response)\n",
    "\n",
    "    memory = []\n",
    "\n",
    "    def turn():\n",
    "        nonlocal game_response\n",
    "        prompt = make_prompt(f'Game prompt:\\n{game_response}')\n",
    "        combined_memory = \"\\n\".join(memory)\n",
    "        prompt_with_memory = f'{combined_memory}\\n\\n{prompt}'\n",
    "        response = ollama.generate(model='llama3.2:1b', prompt=prompt_with_memory).response\n",
    "        print(f'LLM Response: {response}')\n",
    "        player_in = response.splitlines()[-1].strip()\n",
    "\n",
    "        memory.append(f'{prompt}\\n{response}')\n",
    "        if len(memory) > memory_size:\n",
    "            memory.pop(0)\n",
    "\n",
    "        # Take an action in the environment using the step fuction.\n",
    "        # The resulting text-observation, reward, and game-over indicator is returned.\n",
    "        game_response, reward, done, info = env.step(player_in)\n",
    "        game_response = f'Received command: {player_in}\\n{game_response}' # Add text the game received so the LLM can hopefully improve it's formatting\n",
    "        print(game_response)\n",
    "\n",
    "        return done, info\n",
    "\n",
    "    result = n_steps(turn, env)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ye5GGVMk3xWD"
   },
   "outputs": [],
   "source": [
    "basic_llm_with_memory(system_prompt=system_prompt_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdUlmhbP34d3"
   },
   "source": [
    "For the same strategy with memory, we also tried a system prompt generated by ChatGPT to see if it could do any better. It did succeed at giving valid commands more often, but they still were not generally very useful commands. Because it is specified in the prompt, it frequently used the look command, even directly after it already used it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42Wq7SEx3WAI"
   },
   "outputs": [],
   "source": [
    "system_prompt_memory_chatgpt = (\n",
    "    \"You are a player of a classic parser-based interactive fiction game.\\n\"\n",
    "    \"Respond only with a single imperative command in plain lowercase (e.g., â€œlookâ€, â€œgo eastâ€, â€œget keyâ€).\\n\"\n",
    "    \"Do not write sentences, explanations, strategies, or narratives.\\n\"\n",
    "    \"Use only standard text-adventure verbs: look, examine, go, take, drop, open, close, use, talk to, attack, inventory, etc.\\n\"\n",
    "    \"Act rationally based on the gameâ€™s last description.\\n\"\n",
    "    \"If confused, issue â€œlookâ€.\\n\"\n",
    "    \"Notice that the memory you receive contains commands you have issued in the past. Don't repeat commands that won't help you move forward.\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p8TmsIJa30u_"
   },
   "outputs": [],
   "source": [
    "basic_llm_with_memory(system_prompt=system_prompt_memory_chatgpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0XBUJ7Ony8N"
   },
   "source": [
    "We then take a more direct approach to fix the validity of commands. Jericho provides the `env.get_valid_actions()` function that returns a list of valid actions for the current game state (player location, inventory, etc.). We chose to include this list of actions in the modelâ€™s prompt to give it options to choose from. This felt like cheating at first because figuring out the correct commands is part of the gameplay experience of a text adventure game, and ideally we would like to see the model â€œlearnâ€ to play like a human would. The problem is that the only ways we know of to fix the modelâ€™s action outputs are providing the commands like this, training or fine-tuning the model to be better at generating commands, or trying a model other than LLaMa that might have more experience with these types of games. We explore both of these other options later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H71tNMgan--Y"
   },
   "outputs": [],
   "source": [
    "def memory_and_provided_commands(memory_size=5):\n",
    "    system_prompt = (\n",
    "        f'You are a smart video game tester who is skilled at playing old-school text adventure games. You are playing {game}\\n'\n",
    "        'Respond only with a single imperative command in plain lowercase from the list of possible actions below.\\n'\n",
    "        'Do not write sentences, explanations, strategies, or narratives.\\n'\n",
    "        'Act rationally based on the gameâ€™s last description.\\n'\n",
    "        \"Notice that the memory you receive contains commands you have issued in the past. Don't repeat commands that won't help you move forward\\n\"\n",
    "    )\n",
    "\n",
    "    make_prompt = lambda x: f'{system_prompt}\\n\\n{x}'\n",
    "\n",
    "    game_response, info = env.reset()\n",
    "    print(game_response)\n",
    "\n",
    "    memory = []\n",
    "\n",
    "    def turn():\n",
    "        nonlocal game_response\n",
    "        combined_memory = \"\\n\".join(memory)\n",
    "        actions = ', '.join(env.get_valid_actions())\n",
    "        prompt = make_prompt(f'{combined_memory}\\n\\nGame text:\\n{game_response}\\n\\nValid actions: {actions}')\n",
    "        response = ollama.generate(model='llama3.2:1b', prompt=prompt).response\n",
    "        print(f'LLM Response: {response}')\n",
    "        player_in = response.splitlines()[-1].strip()\n",
    "\n",
    "        memory.append(f'{game_response}\\n{response}')\n",
    "        if len(memory) > memory_size:\n",
    "            memory.pop(0)\n",
    "\n",
    "        # Take an action in the environment using the step fuction.\n",
    "        # The resulting text-observation, reward, and game-over indicator is returned.\n",
    "        game_response, reward, done, info = env.step(player_in)\n",
    "        game_response = f'Received command: {player_in}\\n{game_response}' # Add text the game received so the LLM can hopefully improve it's formatting\n",
    "        print(game_response)\n",
    "        return done, info\n",
    "\n",
    "    result = n_steps(turn, env)\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PT9Wzlkn13m"
   },
   "source": [
    "Providing valid actions is another step in the right direction, as it does make the model much more consistent at using valid commands. It still would use an invalid command at times, but it was able to fix its mistake more often by noticing its chosen command wasnâ€™t in the list. The problem this revealed is that the modelâ€™s valid commands are still not necessarily commands that move the game forward. Itâ€™s common in this setup that the model will choose to open the mailbox, take the leaflet, put the leaflet in the mailbox, then repeat the process, or move in random cardinal directions without ever discovering the way into the house. Even when we tell it that its goal is to complete the game, it does not seem to know what that entails, and does not have any sort of â€œcuriosityâ€ to motivate in-depth exploration of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnXpq9hfn4ep"
   },
   "source": [
    "Our next strategy adds a second call to the LLM for every game action. The new prompt has the LLM first analyze the gameâ€™s description and explain the next action to take in natural language. This description is then given back to the model in the second prompt to generate the command alone. We hoped that this could give the model a clearer and more productive goal when choosing its action from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8HsuKoYoD1J"
   },
   "outputs": [],
   "source": [
    "def memory_analyze_provided_commands_chat():\n",
    "    model = 'llama3.2:1b'\n",
    "    system_prompt = (\n",
    "        f'You are a smart video game tester who is skilled at playing old-school text adventure games. You are playing {game}\\n'\n",
    "        'Act rationally based on the gameâ€™s last description.\\n'\n",
    "        \"Notice that the memory you receive contains commands you have issued in the past. Don't repeat commands that won't help you move forward\\n\"\n",
    "    )\n",
    "\n",
    "    make_prompt = lambda x: f'{system_prompt}\\n\\n{x}'\n",
    "    memory = [\n",
    "        ollama.Message(role='system', content=system_prompt)\n",
    "    ]\n",
    "    analysis_prompt = 'Concisely describe the current state of the game and a potential action to take to move forward.'\n",
    "    memory.append(ollama.Message(role='system', content=analysis_prompt))\n",
    "\n",
    "    game_response, info = env.reset()\n",
    "    print(game_response)\n",
    "\n",
    "    def turn_func():\n",
    "        nonlocal model, make_prompt, analysis_prompt, memory, game_response\n",
    "        actions_list = env.get_valid_actions()\n",
    "        random.shuffle(actions_list)\n",
    "        actions = ', '.join(actions_list)\n",
    "\n",
    "        memory.append(ollama.Message(role='user', content=f'{game_response}\\n\\nValid game actions: {actions}'))\n",
    "        response = ollama.chat(model=model, messages=memory)\n",
    "        memory.append(response.message)\n",
    "        response = response.message.content\n",
    "        print(f'LLM description: {response}')\n",
    "\n",
    "        prompt = make_prompt(f'Given your analysis of the game state, issue a rational action to take to progress in the game. Respond only with a single imperative command in plain lowercase. Use only standard text-adventure verbs. IMPORTANT: Your response will be used directly as input to the game. Minimize the number of words you use.\\n\\nYour analysis:\\n{response}\\n\\n Only use one of these valid actions: {actions}\\n\\n')\n",
    "        print('[action prompt]', prompt)\n",
    "        response = ollama.generate(model=model, prompt=prompt).response\n",
    "        print(f'LLM action: {response}')\n",
    "        response = response.removeprefix('type').strip() # Give it a shot. Keeps saying type! TODO: Probably remove. Bandaid\n",
    "\n",
    "        lines = response.splitlines()\n",
    "        player_in = lines[-1].strip() if len(lines) != 0 else ''\n",
    "\n",
    "        # Take an action in the environment using the step fuction.\n",
    "        # The resulting text-observation, reward, and game-over indicator is returned.\n",
    "        game_response, reward, done, info = env.step(player_in)\n",
    "        game_response = f'Received command: {player_in}\\n{game_response}' # Add text the game received so the LLM can hopefully improve it's formatting\n",
    "        print(game_response)\n",
    "\n",
    "        return done, info\n",
    "\n",
    "    results = n_steps(turn_func, env)\n",
    "    print(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01ddRkp4n7AS"
   },
   "source": [
    "Next, we tried using reasoning models on top of our strategy. We tried Qwen3 first, which tended to be very verbose in its thinking, but did perform well compared to LLaMa. We had a similar experience with gpt-oss, but it ran faster on average than Qwen with less meandering in its analysis. With both of these reasoning models, we attempted an agentic workflow, where making actions in game and viewing valid moves are tools the LLM can choose to employ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kFhVSz9X43tz"
   },
   "outputs": [],
   "source": [
    "def agent(max_retry=30, model='qwen3'):\n",
    "\n",
    "    system_prompt = (\n",
    "        f'Think step by step. You are playing {game}, an interactive fiction game. You must analyze the scenario the game presents to you and choose an action that will make progress. Your goal is to finish the game\\n'\n",
    "        'Use the tools provided to you to take actions, view possible actions for your current location, and view the game walkthrough if necessary'\n",
    "    )\n",
    "\n",
    "    game_response, info = env.reset()\n",
    "    print(game_response)\n",
    "    done = False\n",
    "\n",
    "    memory = [\n",
    "        ollama.Message(role='system', content=system_prompt)\n",
    "    ]\n",
    "\n",
    "    def do_game_action(action: str) -> str:\n",
    "        \"\"\"Perform an action in the active text adventure game and see the result\"\"\"\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          action: game action string\n",
    "\n",
    "        Returns:\n",
    "          The game's response after performing the action\n",
    "        \"\"\"\n",
    "        nonlocal done, info\n",
    "        game_response, reward, done, info = env.step(action)\n",
    "        return game_response\n",
    "\n",
    "    def view_possible_actions() -> str:\n",
    "        \"\"\"View a list of the actions that can be performed in the game's current state\"\"\"\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          String containg actions separated by commas\n",
    "        \"\"\"\n",
    "        return ', '.join(env.get_valid_actions())\n",
    "\n",
    "    def view_walkthrough():\n",
    "        \"\"\"View the full game walkthrough as a list of actions\"\"\"\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          String containing actions separated by newlines\n",
    "        \"\"\"\n",
    "        return env.get_walkthrough()\n",
    "\n",
    "    available_functions = {\n",
    "        'do_game_action': do_game_action,\n",
    "        'view_possible_actions': view_possible_actions,\n",
    "        'view_walkthrough': view_walkthrough\n",
    "    }\n",
    "\n",
    "    def turn():\n",
    "        nonlocal game_response, memory\n",
    "        memory.append(ollama.Message(role='user', content=f'{game_response}'))\n",
    "\n",
    "        response = ollama.chat(model=model, messages=memory, think=True, tools=[do_game_action, view_possible_actions, view_walkthrough], options={'num_ctx': 2048})\n",
    "        memory.append(response.message)\n",
    "\n",
    "        print(\"Thinking: \", response.message.thinking)\n",
    "        print(\"Content: \", response.message.content)\n",
    "\n",
    "        if response.message.tool_calls:\n",
    "            for tc in response.message.tool_calls:\n",
    "                if tc.function.name in available_functions:\n",
    "                    print(f\"Calling {tc.function.name} with arguments {tc.function.arguments}\")\n",
    "                    result = available_functions[tc.function.name](**tc.function.arguments)\n",
    "                    print(f\"Result: {result}\")\n",
    "                    # add the tool result to the messages\n",
    "                    memory.append({'role': 'tool', 'tool_name': tc.function.name, 'content': str(result)})\n",
    "        return done, info\n",
    "\n",
    "    result = n_steps(turn, env)\n",
    "    print(result)\n",
    "    print('Memory at end:')\n",
    "    print(json.dumps(memory, indent=4, default=str))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Md4TwogHoJ5u"
   },
   "source": [
    "## Evaluating Existing Knowledge of Zork\n",
    "\n",
    "A question that arises when running these models on a given game is whether the actions they take could just be memorized from being trained on the game in the past. We do acknowledge that memorizing the game is a valid strategy to complete the game, but it would be much more interesting if we could show that a model can be coerced to use logic to make choices and progress in the game. Below, we take the very first message shown to the player by Zork, truncate it around halfway through, and task each model with completing the text. Our assumption is that the more similar the embeddings of the generated text are to the embeddings of the actual text, the more likely that the model has prior experience with the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXulilMSodQ0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_S0LCte5Eao"
   },
   "outputs": [],
   "source": [
    "GAMES_DIR = \"z-machine-games-master/jericho-game-suite\"\n",
    "game = 'zork1.z5'\n",
    "env = jericho.FrotzEnv(f\"{GAMES_DIR}/{game}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR_FDPOP5HhQ"
   },
   "outputs": [],
   "source": [
    "initial_observation, info = env.reset()\n",
    "print(initial_observation)\n",
    "\n",
    "parts = initial_observation.split(' ')\n",
    "middle = len(parts) // 2\n",
    "first_half = ' '.join(parts[:middle+3])\n",
    "second_half = ' '.join(parts[middle+3:])\n",
    "\n",
    "print('first')\n",
    "print(first_half)\n",
    "print()\n",
    "print('second')\n",
    "print(second_half)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs62_UL45J8a"
   },
   "outputs": [],
   "source": [
    "def embedding(text):\n",
    "    embed_result = ollama.embed(model='nomic-embed-text:latest', input=text)\n",
    "    # print(type(embed_result.embeddings))\n",
    "    # print(embed_result.embeddings)\n",
    "    return embed_result.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DI8t4Zcz5LS7"
   },
   "outputs": [],
   "source": [
    "actual = embedding(initial_observation)\n",
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCpSuwSN5NV6"
   },
   "outputs": [],
   "source": [
    "prompt = f'Below is the first half of the very first text prompt given by the text adventure game Zork. Please complete it exactly as it is given by Zork. Do not include anything except your completion.\\n\\n\"{first_half}\" (completion here...)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cehI0Rl5Thb"
   },
   "outputs": [],
   "source": [
    "def test_llama():\n",
    "    llama_response = ollama.generate(model='llama3.2:3b', prompt=prompt).response\n",
    "    rebuilt_llama = first_half + ' ' + llama_response\n",
    "    # print(rebuilt_llama)\n",
    "    llama_embed = embedding(rebuilt_llama)\n",
    "    sim = cosine_similarity(llama_embed, actual)\n",
    "    # print(sim)\n",
    "    return {'text': rebuilt_llama, 'sim': sim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ho2hCqmM5Usf"
   },
   "outputs": [],
   "source": [
    "def test_qwen():\n",
    "    qwen_response = ollama.generate(model='qwen3', prompt=prompt, options={'num_ctx': 2048}).response\n",
    "    rebuilt_qwen = first_half + ' ' + qwen_response\n",
    "    # print(rebuilt_qwen)\n",
    "    qwen_embed = embedding(rebuilt_qwen)\n",
    "    sim = cosine_similarity(qwen_embed, actual)\n",
    "    # print(sim)\n",
    "    return {'text': rebuilt_qwen, 'sim': sim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fgSsfiF5WCM"
   },
   "outputs": [],
   "source": [
    "def test_gpt():\n",
    "    gpt_response = ollama.generate(model='gpt-oss', prompt=prompt, options={'num_ctx': 2048}).response\n",
    "    rebuilt_gpt = first_half + ' ' + gpt_response\n",
    "    # print(rebuilt_gpt)\n",
    "    gptoss_embed = embedding(rebuilt_gpt)\n",
    "    sim = cosine_similarity(gptoss_embed, actual)\n",
    "    # print(sim)\n",
    "    return {'text': rebuilt_gpt, 'sim': sim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XSGcUdU5XMZ"
   },
   "outputs": [],
   "source": [
    "llama_tests = [test_llama() for _ in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jt0FWiIQ5YZi"
   },
   "outputs": [],
   "source": [
    "mean_llama = sum(x['sim'] for x in llama_tests) / 30\n",
    "mean_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqVCBISs5Zch"
   },
   "outputs": [],
   "source": [
    "qwen_tests = [test_qwen() for _ in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZCcgRiv5ap2"
   },
   "outputs": [],
   "source": [
    "mean_qwen = sum(x['sim'] for x in qwen_tests) / 30\n",
    "mean_qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wjMOjLiq5bqi"
   },
   "outputs": [],
   "source": [
    "gpt_tests = [test_gpt() for _ in range(30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bICDdas5dWu"
   },
   "outputs": [],
   "source": [
    "mean_gpt = sum(x['sim'] for x in gpt_tests) / 30\n",
    "mean_gpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qwen had the highest similarity on average, which makes sense intuitively from using it. It often described Zork as if it knew the game, even if it was often wrong in some way. The rest of the models still had high similarity, but each is working with the same starting text and the same system prompt, so it makes sense that the embeddings would be in close proximity in the latent space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pFzY5ZARoTaC"
   },
   "source": [
    "## Q-BERT\n",
    "\n",
    "The Q-BERT model is an existing approach that combines multiple components: an ALBERT model for answering questions about the environment, and another model that constructs commands by combining a knowledge graph with command templates. Unfortunately, we were not able to get Q-BERT working for this project: not only did the code require revision to be compatible with current versions of Python libraries and thus recent GPUs, it also required more memory than we had available on a single machine. We were able to run the ALBERT fine-tuning process, but these memory issues occurred when attempting to train the downstream model.\n",
    "\n",
    "Some parts of Q-BERT were designed to accommodate distributed training, so if we were to revisit this model then we would either want to reduce model/data size or increase the number of machines involved in training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCKx6ld4oqQy"
   },
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Using Low-Rank Adaptation (LoRA), it is possible to fine-tune LLMs by updating a limited subset of their weights. This allows fine-tuning to be accomplished on consumer-grade GPUs with smaller memory. The Unsloth library for Python introduces some of its own optimizations to further accelerate this process.\n",
    "\n",
    "To fine-tune a model, we convert the walkthrough for a game into a dataset containing observations of the environment and the actions the model chooses to take. We train the model to produce these same responses in the same situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import jericho\n",
    "\n",
    "def get_prompt(env: jericho.FrotzEnv, obs: str, done: bool, include_actions: bool = False):\n",
    "    \"\"\" Format a custom prompt including additional information about the environment. \"\"\"\n",
    "    \n",
    "    items=[\"##Observation\\n\" + obs]\n",
    "    state = env.get_state()\n",
    "\n",
    "    if not done:\n",
    "        look_desc, reward, done, info = env.step(\"look\")\n",
    "        if not obs.endswith(look_desc):\n",
    "            items.append(\"##Location\\n\" + look_desc)\n",
    "\n",
    "    if not done:\n",
    "        inv_desc, reward, done, info = env.step(\"inventory\")\n",
    "        items.append(\"##Inventory\\n\" + inv_desc)\n",
    "\n",
    "    if include_actions and not done:\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        bullets = [\"- \" + action for action in valid_actions]\n",
    "        items.append(\"##Available actions\\n\" + \"\\n\".join(bullets))\n",
    "\n",
    "    env.set_state(state)\n",
    "\n",
    "    return \"\\n\\n\".join(items)\n",
    "\n",
    "\n",
    "def get_steps(filename: str, extra_prompt = False):\n",
    "    \"\"\" Return a sequence of (prompt, action) pairs needed to complete a game. \"\"\"\n",
    "    env = jericho.FrotzEnv(filename)\n",
    "    \n",
    "    initial_obs, info = env.reset()\n",
    "    walkthrough = env.get_walkthrough()\n",
    "\n",
    "    steps = []\n",
    "   \n",
    "    done = False\n",
    "    obs = initial_obs\n",
    "    for step in walkthrough:\n",
    "        prompt = get_prompt(env, obs, done, include_actions=False)\n",
    "        steps.append((prompt, step))\n",
    "        obs, reward, done, info = env.step(step)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return steps\n",
    "\n",
    "\n",
    "def steps_to_dataset(steps: list[list[tuple[str, str]]], length: int, overlap: bool = True):\n",
    "    \"\"\"\n",
    "    Convert a sequence of game steps to a dataset of windowed conversations,\n",
    "    where the user prompt is the environment observation and the assistant response\n",
    "    is the command to execute.\n",
    "    \"\"\"\n",
    "    convos = []\n",
    "\n",
    "    for game in steps:\n",
    "        convo = []\n",
    "        n = 0\n",
    "        \n",
    "        for step in game:\n",
    "            convo.append({\"role\": \"user\", \"content\": step[0]})\n",
    "            convo.append({\"role\": \"assistant\", \"content\": step[1]})\n",
    "            n += 1\n",
    "            if overlap:\n",
    "                if length > 0 and n > length:\n",
    "                    n -= 1\n",
    "                    convo.pop(0)\n",
    "                    convo.pop(0)\n",
    "                    \n",
    "                convos.append(list(convo))\n",
    "            else:\n",
    "                if length > 0 and n >= length:\n",
    "                    n = 0\n",
    "                    convos.append(convo)\n",
    "                    convo = []\n",
    "\n",
    "        if len(convo) > 0:\n",
    "            convos.append(convo)\n",
    "\n",
    "    return Dataset.from_dict({\"conversations\": convos})\n",
    "\n",
    "\n",
    "def get_dataset(game_files: list[str], length: int, overlap: bool):\n",
    "    steps = []\n",
    "    for game_file in game_files:\n",
    "        steps.append(get_steps(game_file))\n",
    "    dataset = steps_to_dataset(steps, length=length, overlap=overlap)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def format_dataset(tokenizer, dataset):\n",
    "    \"\"\" Apply the model-specific chat template to a dataset. \"\"\"\n",
    "\n",
    "    # Based on the Unsloth for Llama3.2 notebook located here:\n",
    "    # https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb\n",
    "    def formatting_prompts_func(examples):\n",
    "        convos = examples[\"conversations\"]\n",
    "        texts = [\n",
    "            tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt = False)\n",
    "            for convo in convos\n",
    "        ]\n",
    "        return {'text': texts}\n",
    "\n",
    "    dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can peek at this dataset to see what the output looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': '##Observation\\nCopyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\\nZORK is a registered trademark of Infocom, Inc.\\nRevision 88 / Serial number 840726\\n\\nWest of House\\nYou are standing in an open field west of a white house, with a boarded front door.\\nThere is a small mailbox here.\\n\\n\\n\\n##Inventory\\nYou are empty-handed.\\n\\n',\n",
       "   'role': 'user'},\n",
       "  {'content': 'N', 'role': 'assistant'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_files = [\"./z-machine-games-master/jericho-game-suite/zork1.z5\"]\n",
    "dataset = get_dataset(game_files, length=6, overlap=True)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the base Llama model to fine-tune:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 12-17 14:22:16 [__init__.py:216] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 12-17 14:22:20 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "INFO 12-17 14:22:20 [vllm_utils.py:732] Unsloth: Patching vLLM v0 graph capture\n",
      "==((====))==  Unsloth 2025.12.1: Fast Llama patching. Transformers: 4.57.3. vLLM: 0.10.2.\n",
      "   \\\\   /|    NVIDIA RTX 4000 Ada Generation. Num GPUs = 1. Max memory: 19.548 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/llama-3.2-3b-instruct-bnb-4bit with actual GPU utilization = 49.44%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 19.55 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 32.\n",
      "Unsloth: vLLM's KV Cache can use up to 7.11 GB. Also swap space = 4 GB.\n",
      "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 12-17 14:22:23 [utils.py:328] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 2048, 'enable_prefix_caching': True, 'disable_cascade_attn': True, 'gpu_memory_utilization': 0.4944470289965936, 'max_num_batched_tokens': 4096, 'max_num_seqs': 32, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 64, 'enable_chunked_prefill': True, 'compilation_config': {\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":null,\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":null,\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":null,\"local_cache_dir\":null}, 'model': 'unsloth/llama-3.2-3b-instruct-bnb-4bit'}\n",
      "INFO 12-17 14:22:29 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 14:22:29 [__init__.py:1815] Using max model len 2048\n",
      "WARNING 12-17 14:22:29 [_ipex_ops.py:16] Import error msg: No module named 'intel_extension_for_pytorch'\n",
      "INFO 12-17 14:22:31 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
      "WARNING 12-17 14:22:31 [lora.py:92] `lora_extra_vocab_size` is deprecated and will be removed in v0.12.0. Additional vocabulary support for LoRA adapters is being phased out.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection'], 'llm_int8_threshold': 6.0}\n",
      "INFO 12-17 14:22:32 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='unsloth/llama-3.2-3b-instruct-bnb-4bit', speculative_config=None, tokenizer='unsloth/llama-3.2-3b-instruct-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/llama-3.2-3b-instruct-bnb-4bit, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"inductor\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":false,\"trace.graph_diagram\":false,\"compile_threads\":32,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":64,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 12-17 14:22:32 [parallel_state.py:1165] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 12-17 14:22:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 12-17 14:22:32 [gpu_model_runner.py:2338] Starting to load model unsloth/llama-3.2-3b-instruct-bnb-4bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1217 14:22:32.573120415 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 14:22:32 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "INFO 12-17 14:22:32 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "INFO 12-17 14:22:33 [bitsandbytes_loader.py:758] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 12-17 14:22:33 [weight_utils.py:348] Using model weights format ['*.safetensors']\n",
      "INFO 12-17 14:22:33 [weight_utils.py:406] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890e9e40c9d44878bbbd958dea31804c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95679c27e2be41df87f1f1721fed17e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 14:22:33 [punica_selector.py:19] Using PunicaWrapperGPU.\n",
      "INFO 12-17 14:22:34 [gpu_model_runner.py:2392] Model loading took 2.3519 GiB and 1.103116 seconds\n",
      "INFO 12-17 14:22:39 [backends.py:539] Using cache directory: /s/chopin/a/grad/elewark/.cache/vllm/torch_compile_cache/dc2c8eddc4/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 12-17 14:22:39 [backends.py:550] Dynamo bytecode transform time: 4.96 s\n",
      "INFO 12-17 14:22:42 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.651 s\n",
      "INFO 12-17 14:22:43 [monitor.py:34] torch.compile takes 4.96 s in total\n",
      "INFO 12-17 14:22:44 [gpu_worker.py:298] Available KV cache memory: 6.91 GiB\n",
      "INFO 12-17 14:22:44 [kv_cache_utils.py:864] GPU KV cache size: 64,640 tokens\n",
      "INFO 12-17 14:22:44 [kv_cache_utils.py:868] Maximum concurrency for 2,048 tokens per request: 31.56x\n",
      "INFO 12-17 14:22:44 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:01<00:00,  8.97it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 14:22:46 [gpu_model_runner.py:3118] Graph capturing finished in 2 secs, took 0.35 GiB\n",
      "INFO 12-17 14:22:46 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 2 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 12-17 14:22:47 [gpu_worker.py:391] Free memory on device (19.31/19.55 GiB) on startup. Desired GPU memory utilization is (0.4944470289965936, 9.67 GiB). Actual usage is 2.35 GiB for weight, 0.39 GiB for peak activation, 0.02 GiB for non-torch memory, and 0.35 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=6881795584` to fit into requested memory, or `--kv-cache-memory=17234976256` to fully utilize gpu memory. Current kv cache memory in use is 7414472192 bytes.\n",
      "INFO 12-17 14:22:47 [core.py:218] init engine (profile, create kv cache, warmup model) took 13.09 seconds\n",
      "INFO 12-17 14:22:48 [llm.py:295] Supported_tasks: ('generate',)\n",
      "INFO 12-17 14:22:48 [__init__.py:36] No IOProcessor plugins requested by the model\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'input_layernorm', 'layer_norm1', 'layer_norm2', 'post_layernorm', 'pre_feedforward_layernorm', 'k_norm', 'norm1', 'norm', 'ffn_norm', 'post_attention_layernorm', 'norm2', 'post_feedforward_layernorm', 'attention_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at unsloth/llama-3.2-3b-instruct-bnb-4bit and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['q_norm', 'input_layernorm', 'layer_norm1', 'cross_attn_post_attention_layernorm', 'layer_norm2', 'post_layernorm', 'cross_attn_input_layernorm', 'pre_feedforward_layernorm', 'k_norm', 'norm1', 'norm', 'ffn_norm', 'post_attention_layernorm', 'norm2', 'post_feedforward_layernorm', 'attention_norm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.12.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from adventure.model import load_model, save_model\n",
    "\n",
    "model, tokenizer = load_model(\"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", \"llama-3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, applying the model-specific formatter adds a 'text' attribute containing the actual content the Llama model will see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class 'bytearray_iterator'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot pickle <class 'bytearray_iterator'>: builtins.bytearray_iterator has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Parameter 'function'=<function format_dataset.<locals>.formatting_prompts_func at 0x7f5251ecf420> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n",
      "[datasets.fingerprint|WARNING]Parameter 'function'=<function format_dataset.<locals>.formatting_prompts_func at 0x7f5251ecf420> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only shown once. Subsequent hashing failures won't be shown.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957b8e76d45d4931adf5882ec0c436be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'content': '##Observation\\nCopyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\\nZORK is a registered trademark of Infocom, Inc.\\nRevision 88 / Serial number 840726\\n\\nWest of House\\nYou are standing in an open field west of a white house, with a boarded front door.\\nThere is a small mailbox here.\\n\\n\\n\\n##Inventory\\nYou are empty-handed.\\n\\n',\n",
       "   'role': 'user'},\n",
       "  {'content': 'N', 'role': 'assistant'}],\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n##Observation\\nCopyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\\nZORK is a registered trademark of Infocom, Inc.\\nRevision 88 / Serial number 840726\\n\\nWest of House\\nYou are standing in an open field west of a white house, with a boarded front door.\\nThere is a small mailbox here.\\n\\n\\n\\n##Inventory\\nYou are empty-handed.\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nN<|eot_id|>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = format_dataset(tokenizer, dataset)\n",
    "\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After producing the dataset from the game environment, we can fine-tune the model using Unsloth as follows.\n",
    "The boilerplate logic for fine-tuning is located under the modules directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot locate reference to <class 'bytearray_iterator'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/s/chopin/a/grad/elewark/cs542/cs542-adventure/venv/lib/python3.13/site-packages/dill/_dill.py:422: PicklingWarning: Cannot pickle <class 'bytearray_iterator'>: builtins.bytearray_iterator has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d9f01cc9d54836bcd803af9c684b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=36):   0%|          | 0/397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7499205e0f54f2ba1ae3e9c95585d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=36):   0%|          | 0/397 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 397 | Num Epochs = 1 | Total steps = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:26, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.415200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.795600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.601300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.382400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.751400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.600800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.804000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# More detailed code for fine-tuning and model saving/loading is included in these modules.\n",
    "# That code is based on this tutorial and notebook from Unsloth.\n",
    "# https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/tutorial-how-to-finetune-llama-3-and-use-in-ollama\n",
    "# https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(1B_and_3B)-Conversational.ipynb\n",
    "from adventure.finetune import make_trainer\n",
    "\n",
    "trainer = make_trainer(model, tokenizer, dataset, max_steps=10) # Limit number of steps for this example\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "#save_model(model, tokenizer, \"lora_model_report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copyright (c) 1981, 1982, 1983 Infocom, Inc. All rights reserved.\n",
      "ZORK is a registered trademark of Infocom, Inc.\n",
      "Revision 88 / Serial number 840726\n",
      "\n",
      "West of House\n",
      "You are standing in an open field west of a white house, with a boarded front door.\n",
      "There is a small mailbox here.\n",
      "\n",
      "\n",
      "> E\n",
      "The door is boarded and you can't remove the boards.\n",
      "\n",
      "\n",
      "> E\n",
      "The door is boarded and you can't remove the boards.\n",
      "\n",
      "\n",
      "{'moves': 2, 'unique_rooms': 1, 'unique_hashes': 1, 'unique_items': 0, 'score': 0, 'max_score': 350, 'avg_retries': 1.0, 'avg_generate_time': 0.09915399551391602}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'moves': 2,\n",
       " 'unique_rooms': 1,\n",
       " 'unique_hashes': 1,\n",
       " 'unique_items': 0,\n",
       " 'score': 0,\n",
       " 'max_score': 350,\n",
       " 'avg_retries': 1.0,\n",
       " 'avg_generate_time': 0.09915399551391602}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "from adventure.player import run_game\n",
    "run_game(model, tokenizer, \"./z-machine-games-master/jericho-game-suite/zork1.z5\", 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fine-tuning process is effective at training the model to output commands in the correct format. When trained for more steps than seen here, the model even carries out some correct instructions from the game walkthrough. However, this still does not allow the model to get very far through the game.\n",
    "\n",
    "The main issue here is that this process does not necessarily teach the model how to decide its next action based on the state of the environment. Any deviation from the path it was taught can then compound further since it was not shown how to handle the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpPO6iKFpZpH"
   },
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "In an attempt to improve the generalizability of these models, we tried a reinforcement learning approach. Group-Relative Policy Optimization (GRPO) is a fine-tuning method for LLMs, notably used by DeepSeek to train their reasoning models. This approach simplifies Reinforcement Learning from Human Feedback (RLHF) by removing the value and reward models from the equation.\n",
    "\n",
    "Like other reinforcement learning approaches, GRPO still allows us to define a custom reward function. By setting various criteria, we can encourage commands that increase the score reported by the game environment, pick up items, etc., while discouraging commands that have no effect.\n",
    "\n",
    "Applying GRPO on top of a previously fine-tuned model did marginally improve performance. However, its effectiveness was still limited. One potential reason is that, unlike the training process models using a framework like Gymnasium might undergo, the space of possible situations the model might be in is still constrained by an input dataset. One potential mitigation is to explore the world further than what the walkthrough describes.\n",
    "\n",
    "\n",
    "https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3UCQGB2pjKb"
   },
   "source": [
    "## Retrieval Augmented Generation\n",
    "\n",
    "A recurring issue with language models is that it can be more difficult for them to â€œconnect the dotsâ€ between different observations over the course of the game. Additionally, game worlds can be quite large in size, to the extent where the context size may not be sufficient to capture what information is specifically relevant to a scenario the model encounters.\n",
    "\n",
    "One potential avenue to address these deficiencies is Retrieval Augmented Generation (RAG), which allows a model to search a vector database to find documents similar to a set of keywords. These similarities are computed based on embeddings generated by a separate model. RAG can be further augmented into a graph-based system that allows links between individual documents. This allows more complex relationships to be described, which is useful in this task. For example, edges can describe pathways between locations or objects contained within.\n",
    "\n",
    "Langchain is a framework that can wrap other LLM providers such as Ollama within an API designed for â€œagentâ€-based flows. Here, a model uses sequences of tool actions to carry out some task.\n",
    "\n",
    "Vending-Bench: https://arxiv.org/abs/2502.15840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.documents import Document\n",
    "from ollama import ResponseError\n",
    "\n",
    "def rag_agent():\n",
    "    game_response, info = env.reset()\n",
    "\n",
    "    done = False\n",
    "\n",
    "    @tool(response_format=\"content\")\n",
    "    def do_game_action(action: str) -> str:\n",
    "        \"\"\"Perform an action in the active text adventure game and see the result\"\"\"\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          action: game action string\n",
    "\n",
    "        Returns:\n",
    "          The game's response after performing the action\n",
    "        \"\"\"\n",
    "        nonlocal done, game_response, info\n",
    "        game_response, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            game_response += '\\nYou have finished the game!'\n",
    "        return game_response\n",
    "    \n",
    "    @tool(response_format=\"content\")\n",
    "    def view_possible_actions() -> str:\n",
    "        \"\"\"View a list of the actions that can be performed in the game's current state\"\"\"\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          String containg actions separated by commas\n",
    "        \"\"\"\n",
    "        return ', '.join(env.get_valid_actions())        \n",
    "\n",
    "\n",
    "    tools = [remember, do_game_action, view_possible_actions]\n",
    "    system_prompt = (\n",
    "        f\"You are playing {game}, an interactive fiction game. You must analyze the scenario the game presents to you and choose an action that will make progress. Your goal is to finish the game\\n\"\n",
    "        \"You have access to a tool that allows you to remember past events that have occured in your current playthrough that are relevant to your situation. \"\n",
    "        \"Use the tool to help you decide on the next action to take in-game \"\n",
    "    )\n",
    "    agent = create_agent(model, tools, system_prompt=system_prompt)\n",
    "    \n",
    "    def agent_stream():\n",
    "        nonlocal agent, game_response\n",
    "        query = (\n",
    "            \"Think critically. Finish the game.\\n\"\n",
    "            # f\"Here are relevant items from your past moves:\\n{remember.invoke({'query':game_response})}\\n\"\n",
    "            f\"Here is your current scenario:\\n{game_response}\"\n",
    "        )\n",
    "        try:\n",
    "            for event in agent.stream(\n",
    "                {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "                stream_mode=\"values\",\n",
    "            ):\n",
    "                last_message = event[\"messages\"][-1]\n",
    "                last_message.pretty_print()\n",
    "                \n",
    "                document = Document(\n",
    "                    page_content=last_message.content, metadata={\"move\": info['moves']}\n",
    "                )\n",
    "                insert_into_vector_store([document])\n",
    "                yield None\n",
    "        except ResponseError:\n",
    "            print('ResponseError occurred')\n",
    "            \n",
    "    cur_stream = None\n",
    "    def turn():\n",
    "        nonlocal cur_stream, done, info\n",
    "        if cur_stream is None:\n",
    "            cur_stream = agent_stream()\n",
    "        try:\n",
    "            next(cur_stream)\n",
    "        except StopIteration:\n",
    "            cur_stream = agent_stream()\n",
    "        return done, info\n",
    "    \n",
    "    results = n_steps(turn, env, 10)\n",
    "    print(results)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws_oMb1GqSzS"
   },
   "source": [
    "## Graph-based RAG\n",
    "\n",
    "As seen with Q-BERT, knowledge graphs can be useful to capture the overall state of a game. Individual locations can be represented as nodes on a graph, with directed edges between them indicating pathways. The graph can also link objects within these rooms. By representing connections in this way, spatial relationships can be captured in a way that is accessible to search algorithms, while abstracting away their complexity from the model itself.\n",
    "\n",
    "We used LangChain's Graph RAG retriever, supported by the DataStax [graph-rag](https://datastax.github.io/graph-rag/) library, to use our existing in-memory LangChain vector store as the knowledge graph where edges are defined by commonly keyed metadata fields. The model still provides information about its current game state to the retrieval function, but the response includes the retrieved documents and k connected documents across graph edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adventure.rag_bot import Game as RagGame\n",
    "game = Game()\n",
    "game.run_game()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jj7rk1f9p6n0"
   },
   "source": [
    "## Multi-step Questioning\n",
    "\n",
    "- Describe your current goal\n",
    "\t- Inputs: History\n",
    "- Does this goal align with your overarching goal to complete Zork?\n",
    "\t- Inputs:\n",
    "- List key information you learned after your previous action, and important aspects of your surroundings.\n",
    "\t- Inputs: Room description, history\n",
    "-> RAG\n",
    "- Given what youâ€™ve observed about your current game state, what should be done next?\n",
    "\t- Inputs: Key information, Goal, RAG output\n",
    "- What is your next action?\n",
    "\t- Inputs: What should be done next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MobkQFIoaeL"
   },
   "source": [
    "## Reality Check: GPT-5.2\n",
    "\n",
    "We really wanted to focus on strategies that would be runnable on machines we or another average researcher would have available to them, meaning we wanted to use smaller models that could be hosted locally. However, we would be remiss if we didn't try a larger model. We chose to try GPT-5.2 through OpenAI's API. It is advertised as highly productive model for agentic workflows and task completion. We found that it made it farther through the game than anything else, achieving a score of 40/350 at the highest compared to the rest of the models, which only achieved as high as 15/350. While ChatGPT did make it farther, there was a lot of game left to play and it still either died to the thief or got stuck in a loop of actions somewhere underground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tI6hBsZ5oTFz"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv();\n",
    "\n",
    "game = 'zork1.z5'\n",
    "GAMES_DIR = \"z-machine-games-master/jericho-game-suite\"\n",
    "env = jericho.FrotzEnv(f\"{GAMES_DIR}/{game}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_tracker = ScoreTracker(env)\n",
    "\n",
    "game_response, info = env.reset()\n",
    "print(game_response)\n",
    "done = False\n",
    "\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are the player. You are playing the interactive fiction game Zork. Don't attach the thief\"}\n",
    "]\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    conversation.append({\"role\": \"user\", \"content\": game_response})\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-5.2\",\n",
    "        input=conversation\n",
    "    )\n",
    "\n",
    "    reply = response.output_text\n",
    "    print(\"GPT:\", reply)\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": reply})\n",
    "    \n",
    "    game_response, reward, done, info = env.step(reply)\n",
    "    print(game_response)\n",
    "    \n",
    "    end = time.time()\n",
    "    score_tracker.update(info, start, end)\n",
    "    score_tracker.get_stats(env, info)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Existing Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_copyright = '''West of House\n",
    "You are standing in an open field west of a white house, with a boarded front door.\n",
    "There is a small mailbox here.\n",
    "'''\n",
    "actual = embedding(without_copyright)\n",
    "\n",
    "first_half = '''West of House\n",
    "You are standing in an open field west of\n",
    "'''\n",
    "\n",
    "prompt = f'Below is the first half of the very first text prompt given by the text adventure game Zork. Please complete it exactly as it is given by Zork. Do not include anything except your completion.\\n\\n\"{first_half}\" (completion here...)'\n",
    "response = client.responses.create(\n",
    "        model=\"gpt-5.2\",\n",
    "        input=prompt\n",
    "    )\n",
    "reply = response.output_text\n",
    "rebuilt = first_half + ' ' + reply.split(':')[-1]\n",
    "print(rebuilt)\n",
    "cosine_similarity(embedding(rebuilt), actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-5.2's reconstructed Zork introduction text has the highest cosine similarity of all, often generating it exactly the same as the game. This indicates that its good performance could be linked to it already having memorized Zork or much of Zork, which means it may not be employing any actual reasoning when playing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "| model | Configuration | Unique Rooms | Unique Items | Unique Game States | Score | Average Retries | Average Generate Time |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "|||||||||\n",
    "| llama3.2:3b | Basic LLaMa | 6 | 1 | 13 | 0 | 100+ | 1.85s |\n",
    "| llama3.2:3b | Memory | 5 | 0 | 8 | 10 | 30 | 1.38s |\n",
    "| llama3.2:3b | Memory & ChatGPT Prompt | 8 | 1 | 11 | 0 | 100+ | 0.255s |\n",
    "| llama3.2:3b | Memory & Provided Commands | 8 | 5 | 53 | 10 | 6 | 0.15s |\n",
    "| llama3.2:3b | Memory, Analyze, Provided Commands | 10 | 7 | 60 | 10 | 15.5 | 1.86s |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcInA75xqbJF"
   },
   "source": [
    "## Reverse Conjecture Map\n",
    "\n",
    "![CS542-ConjectureMap.drawio.svg](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>
<!-- Do not edit this file with editors other than draw.io -->
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg xmlns="http://www.w3.org/2000/svg" style="background: #ffffff; background-color: light-dark(#ffffff, var(--ge-dark-color, #121212)); color-scheme: light dark;" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="651px" height="551px" viewBox="0 0 651 551" content="&lt;mxfile host=&quot;app.diagrams.net&quot; agent=&quot;Mozilla/5.0 (X11; Linux x86_64; rv:145.0) Gecko/20100101 Firefox/145.0&quot; version=&quot;29.2.7&quot; scale=&quot;1&quot; border=&quot;0&quot;&gt;&#xA;  &lt;diagram name=&quot;Page-1&quot; id=&quot;KfyWppqO8bjdYQBaH9D3&quot;&gt;&#xA;    &lt;mxGraphModel dx=&quot;949&quot; dy=&quot;720&quot; grid=&quot;1&quot; gridSize=&quot;10&quot; guides=&quot;1&quot; tooltips=&quot;1&quot; connect=&quot;1&quot; arrows=&quot;1&quot; fold=&quot;1&quot; page=&quot;1&quot; pageScale=&quot;1&quot; pageWidth=&quot;850&quot; pageHeight=&quot;1100&quot; math=&quot;0&quot; shadow=&quot;0&quot;&gt;&#xA;      &lt;root&gt;&#xA;        &lt;mxCell id=&quot;0&quot; /&gt;&#xA;        &lt;mxCell id=&quot;1&quot; parent=&quot;0&quot; /&gt;&#xA;        &lt;mxCell id=&quot;5bLvU3Qj09Uloezt2vQZ-2&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;120&quot; width=&quot;180&quot; x=&quot;510&quot; y=&quot;310&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;emBvFD3Pfh0BUBnQb6nT-7&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;160&quot; width=&quot;180&quot; x=&quot;270&quot; y=&quot;200&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;emBvFD3Pfh0BUBnQb6nT-5&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;160&quot; width=&quot;180&quot; x=&quot;270&quot; y=&quot;380&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;emBvFD3Pfh0BUBnQb6nT-3&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;170&quot; width=&quot;180&quot; x=&quot;510&quot; y=&quot;120&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;emBvFD3Pfh0BUBnQb6nT-1&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;140&quot; width=&quot;180&quot; x=&quot;510&quot; y=&quot;450&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-1&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Outcomes&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;40&quot; width=&quot;160&quot; x=&quot;40&quot; y=&quot;40&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-3&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Mediating Processes&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;40&quot; width=&quot;160&quot; x=&quot;280&quot; y=&quot;40&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-4&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Raw Artifacts&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;40&quot; width=&quot;160&quot; x=&quot;520&quot; y=&quot;40&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-20&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;Vlwu5flrsJJmYZA_1hss-5&quot; style=&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;Vlwu5flrsJJmYZA_1hss-11&quot;&gt;&#xA;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-22&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;Vlwu5flrsJJmYZA_1hss-5&quot; style=&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;Vlwu5flrsJJmYZA_1hss-8&quot;&gt;&#xA;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-5&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;LLMs are not good at long-term reasoning on their own&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;60&quot; width=&quot;160&quot; x=&quot;40&quot; y=&quot;440&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-25&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;Vlwu5flrsJJmYZA_1hss-6&quot; style=&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;Vlwu5flrsJJmYZA_1hss-8&quot;&gt;&#xA;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-6&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Text adventure games are an interesting example of a long, complex task&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;60&quot; width=&quot;160&quot; x=&quot;40&quot; y=&quot;520&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-5&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Q-BERT source code&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;40&quot; width=&quot;160&quot; x=&quot;520&quot; y=&quot;130&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-6&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;&amp;lt;div&amp;gt;Unsloth model fine-tuning tutorials/notebooks&amp;lt;/div&amp;gt;&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;40&quot; width=&quot;160&quot; x=&quot;520&quot; y=&quot;180&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-7&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Langchain examples&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;40&quot; width=&quot;160&quot; x=&quot;520&quot; y=&quot;230&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-13&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;5bLvU3Qj09Uloezt2vQZ-2&quot; style=&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0;exitY=0.5;exitDx=0;exitDy=0;entryX=1;entryY=0.5;entryDx=0;entryDy=0;startArrow=classic;startFill=1;endArrow=none;endFill=0;&quot; target=&quot;Vlwu5flrsJJmYZA_1hss-11&quot;&gt;&#xA;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-8&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;&amp;lt;div&amp;gt;Ollama models:&amp;lt;/div&amp;gt;&amp;lt;div&amp;gt;LLaMa, Qwen, gpt-oss&amp;lt;/div&amp;gt;&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;40&quot; width=&quot;160&quot; x=&quot;520&quot; y=&quot;320&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-19&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;Vlwu5flrsJJmYZA_1hss-7&quot; style=&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;Vlwu5flrsJJmYZA_1hss-16&quot;&gt;&#xA;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-7&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;RAG is a convenient way to retrieve relevant information, but does not guarantee better results on its own&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;80&quot; width=&quot;160&quot; x=&quot;40&quot; y=&quot;280&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-9&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Fine-tuning good for specialization, but finding data can be harder&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;60&quot; width=&quot;160&quot; x=&quot;40&quot; y=&quot;200&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-10&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Q-BERT usage hampered by older libraries, memory constraints&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;60&quot; width=&quot;160&quot; x=&quot;40&quot; y=&quot;120&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-24&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;Vlwu5flrsJJmYZA_1hss-8&quot; style=&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;emBvFD3Pfh0BUBnQb6nT-1&quot;&gt;&#xA;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot;&gt;&#xA;            &lt;mxPoint x=&quot;510&quot; y=&quot;420&quot; as=&quot;targetPoint&quot; /&gt;&#xA;          &lt;/mxGeometry&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-8&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Performance metrics based on unique game states&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;60&quot; width=&quot;160&quot; x=&quot;280&quot; y=&quot;460&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-11&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;LoRA + GRPO fine-tuning and RL on walkthroughs&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;60&quot; width=&quot;160&quot; x=&quot;280&quot; y=&quot;210&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-17&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;Vlwu5flrsJJmYZA_1hss-10&quot; style=&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;Vlwu5flrsJJmYZA_1hss-16&quot;&gt;&#xA;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-10&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Content of a prompt can radically change the LLM response&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;60&quot; width=&quot;160&quot; x=&quot;40&quot; y=&quot;370&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-12&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Q-BERT patches for newer Torch/TensorFlow/Jericho&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;60&quot; width=&quot;160&quot; x=&quot;280&quot; y=&quot;120&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-12&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;Vlwu5flrsJJmYZA_1hss-11&quot; style=&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0;exitY=0.5;exitDx=0;exitDy=0;entryX=1;entryY=0.5;entryDx=0;entryDy=0;startArrow=classic;startFill=1;endArrow=none;endFill=0;&quot; target=&quot;Vlwu5flrsJJmYZA_1hss-10&quot;&gt;&#xA;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-11&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Several saved runs of Zork by the LLM&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;60&quot; width=&quot;160&quot; x=&quot;280&quot; y=&quot;387.88&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-20&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;uYqmQ5ttEfTyaAbSHZ2a-10&quot; style=&quot;endArrow=classic;html=1;rounded=0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;uYqmQ5ttEfTyaAbSHZ2a-12&quot; value=&quot;&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;50&quot; relative=&quot;1&quot; width=&quot;50&quot; as=&quot;geometry&quot;&gt;&#xA;            &lt;mxPoint x=&quot;330&quot; y=&quot;380&quot; as=&quot;sourcePoint&quot; /&gt;&#xA;            &lt;mxPoint x=&quot;380&quot; y=&quot;330&quot; as=&quot;targetPoint&quot; /&gt;&#xA;          &lt;/mxGeometry&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-21&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;uYqmQ5ttEfTyaAbSHZ2a-12&quot; style=&quot;endArrow=classic;html=1;rounded=0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;uYqmQ5ttEfTyaAbSHZ2a-5&quot; value=&quot;&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;50&quot; relative=&quot;1&quot; width=&quot;50&quot; as=&quot;geometry&quot;&gt;&#xA;            &lt;mxPoint x=&quot;450&quot; y=&quot;180&quot; as=&quot;sourcePoint&quot; /&gt;&#xA;            &lt;mxPoint x=&quot;500&quot; y=&quot;130&quot; as=&quot;targetPoint&quot; /&gt;&#xA;          &lt;/mxGeometry&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-18&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;Vlwu5flrsJJmYZA_1hss-16&quot; style=&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;5bLvU3Qj09Uloezt2vQZ-2&quot;&gt;&#xA;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-21&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;Vlwu5flrsJJmYZA_1hss-16&quot; style=&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;uYqmQ5ttEfTyaAbSHZ2a-7&quot;&gt;&#xA;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-16&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Implementation of in-memory RAG&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;60&quot; width=&quot;160&quot; x=&quot;280&quot; y=&quot;280&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-22&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;uYqmQ5ttEfTyaAbSHZ2a-11&quot; style=&quot;endArrow=classic;html=1;rounded=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;&quot; target=&quot;uYqmQ5ttEfTyaAbSHZ2a-6&quot; value=&quot;&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;50&quot; relative=&quot;1&quot; width=&quot;50&quot; as=&quot;geometry&quot;&gt;&#xA;            &lt;mxPoint x=&quot;330&quot; y=&quot;380&quot; as=&quot;sourcePoint&quot; /&gt;&#xA;            &lt;mxPoint x=&quot;380&quot; y=&quot;330&quot; as=&quot;targetPoint&quot; /&gt;&#xA;          &lt;/mxGeometry&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-23&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;uYqmQ5ttEfTyaAbSHZ2a-9&quot; style=&quot;endArrow=classic;html=1;rounded=0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;uYqmQ5ttEfTyaAbSHZ2a-11&quot; value=&quot;&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;50&quot; relative=&quot;1&quot; width=&quot;50&quot; as=&quot;geometry&quot;&gt;&#xA;            &lt;mxPoint x=&quot;330&quot; y=&quot;380&quot; as=&quot;sourcePoint&quot; /&gt;&#xA;            &lt;mxPoint x=&quot;380&quot; y=&quot;330&quot; as=&quot;targetPoint&quot; /&gt;&#xA;          &lt;/mxGeometry&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;Vlwu5flrsJJmYZA_1hss-23&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;&amp;lt;div&amp;gt;Microsoft&amp;#39;s Jericho Interface for Learning Agents&amp;lt;/div&amp;gt;&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;60&quot; width=&quot;160&quot; x=&quot;520&quot; y=&quot;460&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-25&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;uYqmQ5ttEfTyaAbSHZ2a-12&quot; style=&quot;endArrow=classic;html=1;rounded=0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;emBvFD3Pfh0BUBnQb6nT-1&quot; value=&quot;&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;50&quot; relative=&quot;1&quot; width=&quot;50&quot; as=&quot;geometry&quot;&gt;&#xA;            &lt;mxPoint x=&quot;280&quot; y=&quot;340&quot; as=&quot;sourcePoint&quot; /&gt;&#xA;            &lt;mxPoint x=&quot;330&quot; y=&quot;290&quot; as=&quot;targetPoint&quot; /&gt;&#xA;          &lt;/mxGeometry&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-26&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;uYqmQ5ttEfTyaAbSHZ2a-11&quot; style=&quot;endArrow=classic;html=1;rounded=0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;uYqmQ5ttEfTyaAbSHZ2a-8&quot; value=&quot;&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;50&quot; relative=&quot;1&quot; width=&quot;50&quot; as=&quot;geometry&quot;&gt;&#xA;            &lt;mxPoint x=&quot;330&quot; y=&quot;380&quot; as=&quot;sourcePoint&quot; /&gt;&#xA;            &lt;mxPoint x=&quot;380&quot; y=&quot;330&quot; as=&quot;targetPoint&quot; /&gt;&#xA;          &lt;/mxGeometry&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-27&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;Zork Z-Machine environment&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;40&quot; width=&quot;160&quot; x=&quot;520&quot; y=&quot;530&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-28&quot; edge=&quot;1&quot; parent=&quot;1&quot; style=&quot;endArrow=classic;html=1;rounded=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;emBvFD3Pfh0BUBnQb6nT-1&quot; value=&quot;&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;50&quot; relative=&quot;1&quot; width=&quot;50&quot; as=&quot;geometry&quot;&gt;&#xA;            &lt;mxPoint x=&quot;440&quot; y=&quot;230&quot; as=&quot;sourcePoint&quot; /&gt;&#xA;            &lt;mxPoint x=&quot;490&quot; y=&quot;270&quot; as=&quot;targetPoint&quot; /&gt;&#xA;          &lt;/mxGeometry&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;uYqmQ5ttEfTyaAbSHZ2a-30&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;Vlwu5flrsJJmYZA_1hss-11&quot; style=&quot;endArrow=classic;html=1;rounded=0;exitX=1;exitY=0.5;exitDx=0;exitDy=0;entryX=0;entryY=0.5;entryDx=0;entryDy=0;&quot; target=&quot;emBvFD3Pfh0BUBnQb6nT-1&quot; value=&quot;&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;50&quot; relative=&quot;1&quot; width=&quot;50&quot; as=&quot;geometry&quot;&gt;&#xA;            &lt;mxPoint x=&quot;440&quot; y=&quot;320&quot; as=&quot;sourcePoint&quot; /&gt;&#xA;            &lt;mxPoint x=&quot;490&quot; y=&quot;460&quot; as=&quot;targetPoint&quot; /&gt;&#xA;          &lt;/mxGeometry&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;emBvFD3Pfh0BUBnQb6nT-6&quot; parent=&quot;1&quot; style=&quot;text;html=1;whiteSpace=wrap;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;rounded=0;&quot; value=&quot;Evaluation&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;20&quot; width=&quot;160&quot; x=&quot;280&quot; y=&quot;520&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;emBvFD3Pfh0BUBnQb6nT-8&quot; parent=&quot;1&quot; style=&quot;text;html=1;whiteSpace=wrap;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;rounded=0;&quot; value=&quot;Refinements&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;20&quot; width=&quot;160&quot; x=&quot;280&quot; y=&quot;340&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;emBvFD3Pfh0BUBnQb6nT-9&quot; parent=&quot;1&quot; style=&quot;text;html=1;whiteSpace=wrap;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;rounded=0;&quot; value=&quot;Base code&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;20&quot; width=&quot;160&quot; x=&quot;520&quot; y=&quot;270&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;emBvFD3Pfh0BUBnQb6nT-10&quot; parent=&quot;1&quot; style=&quot;text;html=1;whiteSpace=wrap;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;rounded=0;&quot; value=&quot;Environment&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;20&quot; width=&quot;160&quot; x=&quot;520&quot; y=&quot;570&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;5bLvU3Qj09Uloezt2vQZ-1&quot; parent=&quot;1&quot; style=&quot;rounded=1;whiteSpace=wrap;html=1;&quot; value=&quot;ChatGPT API&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;40&quot; width=&quot;160&quot; x=&quot;520&quot; y=&quot;370&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;5bLvU3Qj09Uloezt2vQZ-3&quot; edge=&quot;1&quot; parent=&quot;1&quot; source=&quot;5bLvU3Qj09Uloezt2vQZ-2&quot; style=&quot;rounded=0;orthogonalLoop=1;jettySize=auto;html=1;exitX=0;exitY=0.5;exitDx=0;exitDy=0;entryX=1;entryY=0.5;entryDx=0;entryDy=0;startArrow=classic;startFill=1;endArrow=none;endFill=0;&quot; target=&quot;Vlwu5flrsJJmYZA_1hss-8&quot;&gt;&#xA;          &lt;mxGeometry relative=&quot;1&quot; as=&quot;geometry&quot;&gt;&#xA;            &lt;mxPoint x=&quot;540&quot; y=&quot;400&quot; as=&quot;sourcePoint&quot; /&gt;&#xA;            &lt;mxPoint x=&quot;460&quot; y=&quot;478&quot; as=&quot;targetPoint&quot; /&gt;&#xA;          &lt;/mxGeometry&gt;&#xA;        &lt;/mxCell&gt;&#xA;        &lt;mxCell id=&quot;5bLvU3Qj09Uloezt2vQZ-4&quot; parent=&quot;1&quot; style=&quot;text;html=1;whiteSpace=wrap;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;rounded=0;&quot; value=&quot;Large language models&quot; vertex=&quot;1&quot;&gt;&#xA;          &lt;mxGeometry height=&quot;20&quot; width=&quot;160&quot; x=&quot;520&quot; y=&quot;410&quot; as=&quot;geometry&quot; /&gt;&#xA;        &lt;/mxCell&gt;&#xA;      &lt;/root&gt;&#xA;    &lt;/mxGraphModel&gt;&#xA;  &lt;/diagram&gt;&#xA;&lt;/mxfile&gt;&#xA;"><defs/><rect fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212));" width="100%" height="100%" x="0" y="0"/><g><g data-cell-id="0"><g data-cell-id="1"><g data-cell-id="5bLvU3Qj09Uloezt2vQZ-2"><g transform="translate(0.5,0.5)"><rect x="470" y="270" width="180" height="120" rx="18" ry="18" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g></g><g data-cell-id="emBvFD3Pfh0BUBnQb6nT-7"><g transform="translate(0.5,0.5)"><rect x="230" y="160" width="180" height="160" rx="24" ry="24" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g></g><g data-cell-id="emBvFD3Pfh0BUBnQb6nT-5"><g transform="translate(0.5,0.5)"><rect x="230" y="340" width="180" height="160" rx="24" ry="24" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g></g><g data-cell-id="emBvFD3Pfh0BUBnQb6nT-3"><g transform="translate(0.5,0.5)"><rect x="470" y="80" width="180" height="170" rx="25.5" ry="25.5" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g></g><g data-cell-id="emBvFD3Pfh0BUBnQb6nT-1"><g transform="translate(0.5,0.5)"><rect x="470" y="410" width="180" height="140" rx="21" ry="21" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-1"><g transform="translate(0.5,0.5)"><rect x="0" y="0" width="160" height="40" rx="6" ry="6" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 20px; margin-left: 1px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Outcomes</div></div></div></foreignObject><text x="80" y="24" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Outcomes</text></switch></g></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-3"><g transform="translate(0.5,0.5)"><rect x="240" y="0" width="160" height="40" rx="6" ry="6" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 20px; margin-left: 241px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Mediating Processes</div></div></div></foreignObject><text x="320" y="24" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Mediating Processes</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-4"><g transform="translate(0.5,0.5)"><rect x="480" y="0" width="160" height="40" rx="6" ry="6" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 20px; margin-left: 481px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Raw Artifacts</div></div></div></foreignObject><text x="560" y="24" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Raw Artifacts</text></switch></g></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-20"><g transform="translate(0.5,0.5)"><path d="M 160 430 L 234.66 381.36" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 239.06 378.49 L 235.11 385.24 L 234.66 381.36 L 231.29 379.38 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-22"><g transform="translate(0.5,0.5)"><path d="M 160 430 L 233.82 448.46" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 238.92 449.73 L 231.28 451.43 L 233.82 448.46 L 232.97 444.64 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-5"><g transform="translate(0.5,0.5)"><rect x="0" y="400" width="160" height="60" rx="9" ry="9" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 430px; margin-left: 1px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">LLMs are not good at long-term reasoning on their own</div></div></div></foreignObject><text x="80" y="434" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">LLMs are not good at long-...</text></switch></g></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-25"><g transform="translate(0.5,0.5)"><path d="M 160 510 L 234.91 453.82" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 239.11 450.67 L 235.61 457.67 L 234.91 453.82 L 231.41 452.07 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-6"><g transform="translate(0.5,0.5)"><rect x="0" y="480" width="160" height="60" rx="9" ry="9" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 510px; margin-left: 1px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Text adventure games are an interesting example of a long, complex task</div></div></div></foreignObject><text x="80" y="514" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Text adventure games are a...</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-5"><g transform="translate(0.5,0.5)"><rect x="480" y="90" width="160" height="40" rx="6" ry="6" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 110px; margin-left: 481px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Q-BERT source code</div></div></div></foreignObject><text x="560" y="114" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Q-BERT source code</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-6"><g transform="translate(0.5,0.5)"><rect x="480" y="140" width="160" height="40" rx="6" ry="6" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 160px; margin-left: 481px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><div>Unsloth model fine-tuning tutorials/notebooks</div></div></div></div></foreignObject><text x="560" y="164" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Unsloth model fine-tuning...</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-7"><g transform="translate(0.5,0.5)"><rect x="480" y="190" width="160" height="40" rx="6" ry="6" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 210px; margin-left: 481px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Langchain examples</div></div></div></foreignObject><text x="560" y="214" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Langchain examples</text></switch></g></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-13"><g transform="translate(0.5,0.5)"><path d="M 464.74 333.6 L 400 377.88" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 469.08 330.63 L 465.28 337.47 L 464.74 333.6 L 461.32 331.69 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-8"><g transform="translate(0.5,0.5)"><rect x="480" y="280" width="160" height="40" rx="6" ry="6" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 300px; margin-left: 481px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><div>Ollama models:</div><div>LLaMa, Qwen, gpt-oss</div></div></div></div></foreignObject><text x="560" y="304" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Ollama models:...</text></switch></g></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-19"><g transform="translate(0.5,0.5)"><path d="M 160 280 L 233.68 270.79" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 238.89 270.14 L 232.38 274.48 L 233.68 270.79 L 231.51 267.53 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-7"><g transform="translate(0.5,0.5)"><rect x="0" y="240" width="160" height="80" rx="12" ry="12" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 280px; margin-left: 1px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">RAG is a convenient way to retrieve relevant information, but does not guarantee better results on its own</div></div></div></foreignObject><text x="80" y="284" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">RAG is a convenient way to...</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-9"><g transform="translate(0.5,0.5)"><rect x="0" y="160" width="160" height="60" rx="9" ry="9" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 190px; margin-left: 1px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Fine-tuning good for specialization, but finding data can be harder</div></div></div></foreignObject><text x="80" y="194" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Fine-tuning good for speci...</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-10"><g transform="translate(0.5,0.5)"><rect x="0" y="80" width="160" height="60" rx="9" ry="9" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 110px; margin-left: 1px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Q-BERT usage hampered by older libraries, memory constraints</div></div></div></foreignObject><text x="80" y="114" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Q-BERT usage hampered by o...</text></switch></g></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-24"><g transform="translate(0.5,0.5)"><path d="M 400 450 L 464.15 477.49" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 468.97 479.56 L 461.16 480.02 L 464.15 477.49 L 463.92 473.59 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-8"><g transform="translate(0.5,0.5)"><rect x="240" y="420" width="160" height="60" rx="9" ry="9" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 450px; margin-left: 241px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Performance metrics based on unique game states</div></div></div></foreignObject><text x="320" y="454" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Performance metrics based...</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-11"><g transform="translate(0.5,0.5)"><rect x="240" y="170" width="160" height="60" rx="9" ry="9" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 200px; margin-left: 241px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">LoRA + GRPO fine-tuning and RL on walkthroughs</div></div></div></foreignObject><text x="320" y="204" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">LoRA + GRPO fine-tuning an...</text></switch></g></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-17"><g transform="translate(0.5,0.5)"><path d="M 160 360 L 235.77 274.76" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 239.26 270.84 L 237.22 278.39 L 235.77 274.76 L 231.99 273.74 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-10"><g transform="translate(0.5,0.5)"><rect x="0" y="330" width="160" height="60" rx="9" ry="9" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 360px; margin-left: 1px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Content of a prompt can radically change the LLM response</div></div></div></foreignObject><text x="80" y="364" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Content of a prompt can ra...</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-12"><g transform="translate(0.5,0.5)"><rect x="240" y="80" width="160" height="60" rx="9" ry="9" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 110px; margin-left: 241px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Q-BERT patches for newer Torch/TensorFlow/Jericho</div></div></div></foreignObject><text x="320" y="114" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Q-BERT patches for newer T...</text></switch></g></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-12"><g transform="translate(0.5,0.5)"><path d="M 233.79 376.49 L 160 360" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 238.91 377.64 L 231.31 379.53 L 233.79 376.49 L 232.84 372.69 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-11"><g transform="translate(0.5,0.5)"><rect x="240" y="347.88" width="160" height="60" rx="9" ry="9" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 378px; margin-left: 241px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Several saved runs of Zork by the LLM</div></div></div></foreignObject><text x="320" y="381" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Several saved runs of Zork...</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-20"><g transform="translate(0.5,0.5)"><path d="M 160 110 L 233.63 110" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 238.88 110 L 231.88 113.5 L 233.63 110 L 231.88 106.5 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-21"><g transform="translate(0.5,0.5)"><path d="M 400 110 L 473.63 110" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 478.88 110 L 471.88 113.5 L 473.63 110 L 471.88 106.5 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-18"><g transform="translate(0.5,0.5)"><path d="M 400 270 L 465.17 325.86" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 469.15 329.27 L 461.56 327.37 L 465.17 325.86 L 466.11 322.06 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-21"><g transform="translate(0.5,0.5)"><path d="M 400 270 L 474.91 213.82" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 479.11 210.67 L 475.61 217.67 L 474.91 213.82 L 471.41 212.07 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-16"><g transform="translate(0.5,0.5)"><rect x="240" y="240" width="160" height="60" rx="9" ry="9" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 270px; margin-left: 241px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Implementation of in-memory RAG</div></div></div></foreignObject><text x="320" y="274" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Implementation of in-memor...</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-22"><g transform="translate(0.5,0.5)"><path d="M 400 200 L 474.3 162.85" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 479 160.5 L 474.3 166.76 L 474.3 162.85 L 471.17 160.5 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-23"><g transform="translate(0.5,0.5)"><path d="M 160 190 L 233.68 199.21" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 238.89 199.86 L 231.51 202.47 L 233.68 199.21 L 232.38 195.52 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="Vlwu5flrsJJmYZA_1hss-23"><g transform="translate(0.5,0.5)"><rect x="480" y="420" width="160" height="60" rx="9" ry="9" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 450px; margin-left: 481px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; "><div>Microsoft's Jericho Interface for Learning Agents</div></div></div></div></foreignObject><text x="560" y="454" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Microsoft's Jericho Interf...</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-25"><g transform="translate(0.5,0.5)"><path d="M 400 110 L 468.82 473.74" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 469.79 478.9 L 465.05 472.67 L 468.82 473.74 L 471.93 471.37 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-26"><g transform="translate(0.5,0.5)"><path d="M 400 200 L 476.02 295.03" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 479.3 299.13 L 472.2 295.85 L 476.02 295.03 L 477.66 291.47 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-27"><g transform="translate(0.5,0.5)"><rect x="480" y="490" width="160" height="40" rx="6" ry="6" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 510px; margin-left: 481px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Zork Z-Machine environment</div></div></div></foreignObject><text x="560" y="514" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Zork Z-Machine environment</text></switch></g></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-28"><g transform="translate(0.5,0.5)"><path d="M 400 190 L 468.51 473.81" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 469.74 478.91 L 464.69 472.93 L 468.51 473.81 L 471.5 471.29 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="uYqmQ5ttEfTyaAbSHZ2a-30"><g transform="translate(0.5,0.5)"><path d="M 400 377.88 L 466.4 474.75" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 469.37 479.08 L 462.52 475.28 L 466.4 474.75 L 468.3 471.33 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="emBvFD3Pfh0BUBnQb6nT-6"><g transform="translate(0.5,0.5)"><rect x="240" y="480" width="160" height="20" fill="none" stroke="none" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 490px; margin-left: 241px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Evaluation</div></div></div></foreignObject><text x="320" y="494" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Evaluation</text></switch></g></g></g><g data-cell-id="emBvFD3Pfh0BUBnQb6nT-8"><g transform="translate(0.5,0.5)"><rect x="240" y="300" width="160" height="20" fill="none" stroke="none" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 310px; margin-left: 241px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Refinements</div></div></div></foreignObject><text x="320" y="314" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Refinements</text></switch></g></g></g><g data-cell-id="emBvFD3Pfh0BUBnQb6nT-9"><g transform="translate(0.5,0.5)"><rect x="480" y="230" width="160" height="20" fill="none" stroke="none" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 240px; margin-left: 481px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Base code</div></div></div></foreignObject><text x="560" y="244" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Base code</text></switch></g></g></g><g data-cell-id="emBvFD3Pfh0BUBnQb6nT-10"><g transform="translate(0.5,0.5)"><rect x="480" y="530" width="160" height="20" fill="none" stroke="none" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 540px; margin-left: 481px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Environment</div></div></div></foreignObject><text x="560" y="544" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Environment</text></switch></g></g></g><g data-cell-id="5bLvU3Qj09Uloezt2vQZ-1"><g transform="translate(0.5,0.5)"><rect x="480" y="330" width="160" height="40" rx="6" ry="6" fill="#ffffff" style="fill: light-dark(#ffffff, var(--ge-dark-color, #121212)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 350px; margin-left: 481px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">ChatGPT API</div></div></div></foreignObject><text x="560" y="354" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">ChatGPT API</text></switch></g></g></g><g data-cell-id="5bLvU3Qj09Uloezt2vQZ-3"><g transform="translate(0.5,0.5)"><path d="M 466.79 335.5 L 400 450" fill="none" stroke="#000000" style="stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke-miterlimit="10" pointer-events="stroke"/><path d="M 469.44 330.97 L 468.93 338.78 L 466.79 335.5 L 462.89 335.25 Z" fill="#000000" style="fill: light-dark(rgb(0, 0, 0), rgb(255, 255, 255)); stroke: light-dark(rgb(0, 0, 0), rgb(255, 255, 255));" stroke="#000000" stroke-miterlimit="10" pointer-events="all"/></g></g><g data-cell-id="5bLvU3Qj09Uloezt2vQZ-4"><g transform="translate(0.5,0.5)"><rect x="480" y="370" width="160" height="20" fill="none" stroke="none" pointer-events="all"/></g><g><g><switch><foreignObject style="overflow: visible; text-align: left;" pointer-events="none" width="100%" height="100%" requiredFeatures="http://www.w3.org/TR/SVG11/feature#Extensibility"><div xmlns="http://www.w3.org/1999/xhtml" style="display: flex; align-items: unsafe center; justify-content: unsafe center; width: 158px; height: 1px; padding-top: 380px; margin-left: 481px;"><div style="box-sizing: border-box; font-size: 0; text-align: center; color: #000000; "><div style="display: inline-block; font-size: 12px; font-family: Helvetica; color: light-dark(#000000, #ffffff); line-height: 1.2; pointer-events: all; white-space: normal; word-wrap: normal; ">Large language models</div></div></div></foreignObject><text x="560" y="384" fill="light-dark(#000000, #ffffff)" font-family="Helvetica" font-size="12px" text-anchor="middle">Large language models</text></switch></g></g></g></g></g></g></svg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me_P49wfrxVh"
   },
   "source": [
    "## Apportionment of Work\n",
    "\n",
    "| Person | Tasks |\n",
    "| --- | --- |\n",
    "| Everett | Off-the-shelf LLM prototype, Q-BERT, LoRA, GRPO, Graph RAG |\n",
    "| Tyson | Off-the-shelf LLMs, GPT-5 experiment, Retrieval Augmented Generation, Multi-Step Questioning, RAG |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
